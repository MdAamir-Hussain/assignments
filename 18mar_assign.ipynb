{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfefb95b-0984-4a1c-8c2d-019a8f59b824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The filter method is a popular approach in feature selection, which selects the most relevant features based on statistical methods or other pre-defined criteria. The filter method works by evaluating the individual features and their relationship with the target variable without considering the interaction among the features.\\n\\nThe filter method uses statistical tests such as chi-square, correlation, mutual information, and ANOVA to calculate the relevance of each feature with respect to the target variable. The features are then ranked based on their scores, and the top-ranked features are selected for further analysis.\\n\\nThe advantages of using the filter method are that it is computationally efficient, easy to implement, and can handle a large number of features. However, the filter method has limitations, as it does not consider the interdependence among features, and the selected features may not always be the most relevant for the specific model or problem.\\n\\nTherefore, it is recommended to use a combination of feature selection methods, including filter, wrapper, and embedded methods, to identify the best subset of features that can improve the model's performance.\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#1\n",
    "'''The filter method is a popular approach in feature selection, which selects the most relevant features based on statistical methods or other pre-defined criteria. The filter method works by evaluating the individual features and their relationship with the target variable without considering the interaction among the features.\n",
    "\n",
    "The filter method uses statistical tests such as chi-square, correlation, mutual information, and ANOVA to calculate the relevance of each feature with respect to the target variable. The features are then ranked based on their scores, and the top-ranked features are selected for further analysis.\n",
    "\n",
    "The advantages of using the filter method are that it is computationally efficient, easy to implement, and can handle a large number of features. However, the filter method has limitations, as it does not consider the interdependence among features, and the selected features may not always be the most relevant for the specific model or problem.\n",
    "\n",
    "Therefore, it is recommended to use a combination of feature selection methods, including filter, wrapper, and embedded methods, to identify the best subset of features that can improve the model's performance.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5748e969-1c22-4345-b2bb-103ae9f58096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Wrapper method in feature selection is different from the Filter method in that it evaluates the performance of a specific machine learning model with different subsets of features, rather than relying on pre-defined statistical measures of feature relevance.\\n\\nThe Wrapper method works by selecting subsets of features and then training a machine learning model on each subset to evaluate its performance. The performance of the model is then used to determine the importance of the selected features. This process is repeated for different subsets of features until the best performing subset is identified.\\n\\nThe main advantage of the Wrapper method is that it can identify the best subset of features for a specific machine learning model and problem, as it takes into account the interaction among the features. However, the Wrapper method can be computationally expensive, especially when dealing with a large number of features or models.\\n\\nIn contrast, the Filter method selects features based on pre-defined statistical measures, such as chi-square, correlation, or mutual information, without considering the interaction among features or the specific machine learning model used. The Filter method is computationally efficient but may not always select the most relevant features for a specific problem.\\n\\nIn summary, the Wrapper method evaluates the performance of a specific machine learning model with different subsets of features, while the Filter method selects features based on pre-defined statistical measures without considering the interaction among features or the specific machine learning model used.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2.\n",
    "'''The Wrapper method in feature selection is different from the Filter method in that it evaluates the performance of a specific machine learning model with different subsets of features, rather than relying on pre-defined statistical measures of feature relevance.\n",
    "\n",
    "The Wrapper method works by selecting subsets of features and then training a machine learning model on each subset to evaluate its performance. The performance of the model is then used to determine the importance of the selected features. This process is repeated for different subsets of features until the best performing subset is identified.\n",
    "\n",
    "The main advantage of the Wrapper method is that it can identify the best subset of features for a specific machine learning model and problem, as it takes into account the interaction among the features. However, the Wrapper method can be computationally expensive, especially when dealing with a large number of features or models.\n",
    "\n",
    "In contrast, the Filter method selects features based on pre-defined statistical measures, such as chi-square, correlation, or mutual information, without considering the interaction among features or the specific machine learning model used. The Filter method is computationally efficient but may not always select the most relevant features for a specific problem.\n",
    "\n",
    "In summary, the Wrapper method evaluates the performance of a specific machine learning model with different subsets of features, while the Filter method selects features based on pre-defined statistical measures without considering the interaction among features or the specific machine learning model used.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4505f1aa-2a8c-4417-a7dd-28b598369a86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Embedded feature selection methods are a class of feature selection techniques that select the most relevant features during the training process of a machine learning algorithm. Here are some common techniques used in Embedded feature selection methods:\\n\\nRegularization: Regularization techniques, such as Lasso and Ridge regression, are used to introduce a penalty term that shrinks the coefficient values of irrelevant features towards zero, resulting in a sparse set of features that are relevant to the model.\\n\\nDecision Trees: Decision trees can be used as an embedded feature selection method by measuring the importance of each feature based on its contribution to the decision tree's splits. Features with high importance are selected for further analysis, while less important features are pruned.\\n\\nGradient Boosting Machines: Gradient Boosting Machines (GBMs) are machine learning algorithms that can perform feature selection by using gradient boosting to optimize the weights of the features. Features with lower weights are deemed less important and are eliminated from the model.\\n\\nNeural Networks: Neural networks can also perform embedded feature selection by adjusting the weights of the features during the training process. The weights of the irrelevant features are reduced, while the weights of the important features are increased.\\n\\nSupport Vector Machines: Support Vector Machines (SVMs) can use regularization techniques such as L1 regularization to eliminate irrelevant features. SVMs can also use kernel functions to map the original features to a higher-dimensional space, where the relevant features become more separable.\\n\\nThese are some of the common techniques used in Embedded feature selection methods. Embedded methods offer the advantage of selecting the most relevant features during the training process, resulting in a more accurate and efficient model.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3.\n",
    "'''Embedded feature selection methods are a class of feature selection techniques that select the most relevant features during the training process of a machine learning algorithm. Here are some common techniques used in Embedded feature selection methods:\n",
    "\n",
    "Regularization: Regularization techniques, such as Lasso and Ridge regression, are used to introduce a penalty term that shrinks the coefficient values of irrelevant features towards zero, resulting in a sparse set of features that are relevant to the model.\n",
    "\n",
    "Decision Trees: Decision trees can be used as an embedded feature selection method by measuring the importance of each feature based on its contribution to the decision tree's splits. Features with high importance are selected for further analysis, while less important features are pruned.\n",
    "\n",
    "Gradient Boosting Machines: Gradient Boosting Machines (GBMs) are machine learning algorithms that can perform feature selection by using gradient boosting to optimize the weights of the features. Features with lower weights are deemed less important and are eliminated from the model.\n",
    "\n",
    "Neural Networks: Neural networks can also perform embedded feature selection by adjusting the weights of the features during the training process. The weights of the irrelevant features are reduced, while the weights of the important features are increased.\n",
    "\n",
    "Support Vector Machines: Support Vector Machines (SVMs) can use regularization techniques such as L1 regularization to eliminate irrelevant features. SVMs can also use kernel functions to map the original features to a higher-dimensional space, where the relevant features become more separable.\n",
    "\n",
    "These are some of the common techniques used in Embedded feature selection methods. Embedded methods offer the advantage of selecting the most relevant features during the training process, resulting in a more accurate and efficient model.'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2684b4bf-ddd8-4e5f-a4ef-d1207d3ac39f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Although the Filter method is a popular approach for feature selection due to its simplicity and efficiency, it has some drawbacks that researchers and practitioners should be aware of:\\n\\nLimited to statistical measures: The Filter method relies on statistical measures such as correlation, mutual information, and chi-square to evaluate the relevance of features. These measures do not take into account the interaction among features or the specific machine learning model used, which may result in the selection of irrelevant features or missing important features.\\n\\nUnable to capture complex relationships: The Filter method may not capture complex relationships among features, such as nonlinear or high-order interactions. These complex relationships may be important for accurate modeling and prediction, but the Filter method cannot identify them.\\n\\nPredefined thresholds: The Filter method uses predefined thresholds to select features based on their scores. These thresholds may be arbitrary and may not reflect the specific problem or machine learning model used. Setting the threshold too low may result in selecting irrelevant features, while setting it too high may exclude important features.\\n\\nNot adaptive: The Filter method does not adapt to changes in the data or the model. The selected features may not be optimal for different datasets or models, and the Filter method needs to be rerun for each new dataset or model.\\n\\nInefficient for large datasets: The Filter method may be computationally inefficient for large datasets with many features, as it requires calculating statistical measures for each feature. This can result in long processing times and may require reducing the number of features before applying the Filter method.\\n\\nIn summary, the Filter method has some limitations that researchers and practitioners should be aware of when using it for feature selection. The Filter method may not capture complex relationships, be adaptive, or efficient for large datasets, and may require arbitrary threshold values.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4.\n",
    "'''Although the Filter method is a popular approach for feature selection due to its simplicity and efficiency, it has some drawbacks that researchers and practitioners should be aware of:\n",
    "\n",
    "Limited to statistical measures: The Filter method relies on statistical measures such as correlation, mutual information, and chi-square to evaluate the relevance of features. These measures do not take into account the interaction among features or the specific machine learning model used, which may result in the selection of irrelevant features or missing important features.\n",
    "\n",
    "Unable to capture complex relationships: The Filter method may not capture complex relationships among features, such as nonlinear or high-order interactions. These complex relationships may be important for accurate modeling and prediction, but the Filter method cannot identify them.\n",
    "\n",
    "Predefined thresholds: The Filter method uses predefined thresholds to select features based on their scores. These thresholds may be arbitrary and may not reflect the specific problem or machine learning model used. Setting the threshold too low may result in selecting irrelevant features, while setting it too high may exclude important features.\n",
    "\n",
    "Not adaptive: The Filter method does not adapt to changes in the data or the model. The selected features may not be optimal for different datasets or models, and the Filter method needs to be rerun for each new dataset or model.\n",
    "\n",
    "Inefficient for large datasets: The Filter method may be computationally inefficient for large datasets with many features, as it requires calculating statistical measures for each feature. This can result in long processing times and may require reducing the number of features before applying the Filter method.\n",
    "\n",
    "In summary, the Filter method has some limitations that researchers and practitioners should be aware of when using it for feature selection. The Filter method may not capture complex relationships, be adaptive, or efficient for large datasets, and may require arbitrary threshold values.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d95660ea-404e-4168-a602-95da6aba3435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are several situations where using the Filter method for feature selection may be preferable over the Wrapper method:\\n\\nLarge datasets: The Filter method is computationally efficient and can handle large datasets with many features. In contrast, the Wrapper method may be computationally expensive and may require reducing the number of features before applying the method.\\n\\nPreprocessing: The Filter method is a simple and straightforward technique that can be applied before preprocessing the data. This allows for an initial feature selection before performing more complex data preprocessing techniques such as normalization, scaling, or feature engineering.\\n\\nStatistical significance: The Filter method relies on statistical measures such as correlation, mutual information, and chi-square to evaluate the relevance of features. These measures provide a measure of statistical significance that can be useful in identifying important features and removing noise.\\n\\nInterpretability: The Filter method is based on statistical measures that are easy to understand and interpret. This makes it a suitable method for situations where interpretability is important, such as in healthcare or finance.\\n\\nGeneralization: The Filter method selects features based on their relevance to the data, rather than the specific machine learning model used. This allows for a generalization of the selected features across different models and datasets.\\n\\nIn summary, the Filter method may be preferred over the Wrapper method when dealing with large datasets, when simple and interpretable statistical measures are required, or when generalization across different models is desired. However, it is important to be aware of the limitations of the Filter method and to evaluate its performance for each specific problem and dataset.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5.\n",
    "'''There are several situations where using the Filter method for feature selection may be preferable over the Wrapper method:\n",
    "\n",
    "Large datasets: The Filter method is computationally efficient and can handle large datasets with many features. In contrast, the Wrapper method may be computationally expensive and may require reducing the number of features before applying the method.\n",
    "\n",
    "Preprocessing: The Filter method is a simple and straightforward technique that can be applied before preprocessing the data. This allows for an initial feature selection before performing more complex data preprocessing techniques such as normalization, scaling, or feature engineering.\n",
    "\n",
    "Statistical significance: The Filter method relies on statistical measures such as correlation, mutual information, and chi-square to evaluate the relevance of features. These measures provide a measure of statistical significance that can be useful in identifying important features and removing noise.\n",
    "\n",
    "Interpretability: The Filter method is based on statistical measures that are easy to understand and interpret. This makes it a suitable method for situations where interpretability is important, such as in healthcare or finance.\n",
    "\n",
    "Generalization: The Filter method selects features based on their relevance to the data, rather than the specific machine learning model used. This allows for a generalization of the selected features across different models and datasets.\n",
    "\n",
    "In summary, the Filter method may be preferred over the Wrapper method when dealing with large datasets, when simple and interpretable statistical measures are required, or when generalization across different models is desired. However, it is important to be aware of the limitations of the Filter method and to evaluate its performance for each specific problem and dataset.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f730d182-6d79-4584-aea0-6afc4f1e5e9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To choose the most pertinent attributes for the predictive model using the Filter Method, you can follow these steps:\\n\\nDefine the target variable: In this case, the target variable is customer churn, which is the variable that you want to predict.\\n\\nSelect the candidate features: Identify all the candidate features that could potentially be useful in predicting customer churn. These features may include demographic data, usage patterns, payment history, service complaints, etc.\\n\\nAnalyze feature correlations: Use statistical measures such as correlation or mutual information to identify the features that are highly correlated with customer churn. These highly correlated features are likely to be more relevant and informative for the predictive model.\\n\\nAnalyze feature importance: Use feature selection algorithms such as chi-square or information gain to calculate the importance of each feature in predicting customer churn. The features with the highest importance scores are likely to be the most pertinent attributes for the predictive model.\\n\\nCheck for redundancy: Check for redundancy among the selected features by analyzing the pairwise correlations between them. Redundant features may not provide additional information and can be removed from the model.\\n\\nEvaluate the model: Once you have selected the most pertinent attributes for the predictive model, evaluate the model's performance using the selected features. If the model's performance is satisfactory, then you can use these features for predicting customer churn.\\n\\nIn summary, to choose the most pertinent attributes for the predictive model using the Filter Method, you need to identify the candidate features, analyze their correlations and importance, check for redundancy, and evaluate the model's performance using the selected features.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#6.\n",
    "'''To choose the most pertinent attributes for the predictive model using the Filter Method, you can follow these steps:\n",
    "\n",
    "Define the target variable: In this case, the target variable is customer churn, which is the variable that you want to predict.\n",
    "\n",
    "Select the candidate features: Identify all the candidate features that could potentially be useful in predicting customer churn. These features may include demographic data, usage patterns, payment history, service complaints, etc.\n",
    "\n",
    "Analyze feature correlations: Use statistical measures such as correlation or mutual information to identify the features that are highly correlated with customer churn. These highly correlated features are likely to be more relevant and informative for the predictive model.\n",
    "\n",
    "Analyze feature importance: Use feature selection algorithms such as chi-square or information gain to calculate the importance of each feature in predicting customer churn. The features with the highest importance scores are likely to be the most pertinent attributes for the predictive model.\n",
    "\n",
    "Check for redundancy: Check for redundancy among the selected features by analyzing the pairwise correlations between them. Redundant features may not provide additional information and can be removed from the model.\n",
    "\n",
    "Evaluate the model: Once you have selected the most pertinent attributes for the predictive model, evaluate the model's performance using the selected features. If the model's performance is satisfactory, then you can use these features for predicting customer churn.\n",
    "\n",
    "In summary, to choose the most pertinent attributes for the predictive model using the Filter Method, you need to identify the candidate features, analyze their correlations and importance, check for redundancy, and evaluate the model's performance using the selected features.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eeb1d60c-4a2a-4cda-a020-1ca2dd731adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To use the Embedded method to select the most relevant features for predicting the outcome of a soccer match, you can follow these steps:\\n\\nChoose a machine learning algorithm: The Embedded method requires a machine learning algorithm that can perform feature selection during training. Algorithms such as LASSO (Least Absolute Shrinkage and Selection Operator), Ridge Regression, or Elastic Net are commonly used in the Embedded method.\\n\\nDefine the target variable: The target variable in this case is the outcome of the soccer match, which can be binary (win/loss) or multiclass (win/draw/loss).\\n\\nPrepare the dataset: Prepare the dataset by cleaning, preprocessing, and scaling the data. Remove any missing values or outliers and encode categorical variables.\\n\\nTrain the machine learning algorithm: Train the machine learning algorithm using the entire dataset, including all the features.\\n\\nAnalyze the feature coefficients: After training the algorithm, analyze the feature coefficients to identify the most relevant features. The feature coefficients indicate the strength and direction of the relationship between each feature and the target variable.\\n\\nRemove irrelevant features: Remove the features with the lowest feature coefficients or set them to zero, indicating their insignificance in predicting the target variable. This reduces the number of features and improves the model's performance.\\n\\nTune hyperparameters: Tune the hyperparameters of the machine learning algorithm to optimize the model's performance using cross-validation.\\n\\nEvaluate the model: Evaluate the performance of the model using the selected features and tuned hyperparameters. Use metrics such as accuracy, precision, recall, F1 score, or AUC-ROC to evaluate the model's performance.\\n\\nIn summary, to use the Embedded method to select the most relevant features for predicting the outcome of a soccer match, you need to choose a suitable machine learning algorithm, define the target variable, prepare the dataset, train the algorithm, analyze the feature coefficients, remove irrelevant features, tune the hyperparameters, and evaluate the model's performance.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#7.\n",
    "'''To use the Embedded method to select the most relevant features for predicting the outcome of a soccer match, you can follow these steps:\n",
    "\n",
    "Choose a machine learning algorithm: The Embedded method requires a machine learning algorithm that can perform feature selection during training. Algorithms such as LASSO (Least Absolute Shrinkage and Selection Operator), Ridge Regression, or Elastic Net are commonly used in the Embedded method.\n",
    "\n",
    "Define the target variable: The target variable in this case is the outcome of the soccer match, which can be binary (win/loss) or multiclass (win/draw/loss).\n",
    "\n",
    "Prepare the dataset: Prepare the dataset by cleaning, preprocessing, and scaling the data. Remove any missing values or outliers and encode categorical variables.\n",
    "\n",
    "Train the machine learning algorithm: Train the machine learning algorithm using the entire dataset, including all the features.\n",
    "\n",
    "Analyze the feature coefficients: After training the algorithm, analyze the feature coefficients to identify the most relevant features. The feature coefficients indicate the strength and direction of the relationship between each feature and the target variable.\n",
    "\n",
    "Remove irrelevant features: Remove the features with the lowest feature coefficients or set them to zero, indicating their insignificance in predicting the target variable. This reduces the number of features and improves the model's performance.\n",
    "\n",
    "Tune hyperparameters: Tune the hyperparameters of the machine learning algorithm to optimize the model's performance using cross-validation.\n",
    "\n",
    "Evaluate the model: Evaluate the performance of the model using the selected features and tuned hyperparameters. Use metrics such as accuracy, precision, recall, F1 score, or AUC-ROC to evaluate the model's performance.\n",
    "\n",
    "In summary, to use the Embedded method to select the most relevant features for predicting the outcome of a soccer match, you need to choose a suitable machine learning algorithm, define the target variable, prepare the dataset, train the algorithm, analyze the feature coefficients, remove irrelevant features, tune the hyperparameters, and evaluate the model's performance.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef5ba5bc-d9b0-4790-86f4-67ab3aa35e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To use the Wrapper method to select the best set of features for predicting the price of a house, you can follow these steps:\\n\\nSelect a subset of features: Start by selecting a subset of features based on domain knowledge or intuition. You can also use all the available features in the dataset.\\n\\nTrain a machine learning algorithm: Train a machine learning algorithm using the selected features and evaluate its performance using a suitable metric, such as mean squared error or R-squared.\\n\\nPerform feature selection: Perform feature selection by recursively adding or removing features from the subset and retraining the machine learning algorithm. This process continues until the optimal set of features is found.\\n\\nChoose a stopping criterion: Choose a stopping criterion, such as a maximum number of iterations or a minimum improvement in performance, to avoid overfitting or underfitting.\\n\\nEvaluate the model: Once the optimal set of features is found, evaluate the model's performance using cross-validation or a separate validation dataset to estimate its generalization ability.\\n\\nIn summary, to use the Wrapper method to select the best set of features for predicting the price of a house, you need to start with a subset of features, train a machine learning algorithm, perform feature selection by recursively adding or removing features, choose a stopping criterion, and evaluate the model's performance. The Wrapper method is computationally expensive and may suffer from overfitting, but it can potentially identify the best set of features for the predictor.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#8.\n",
    "'''To use the Wrapper method to select the best set of features for predicting the price of a house, you can follow these steps:\n",
    "\n",
    "Select a subset of features: Start by selecting a subset of features based on domain knowledge or intuition. You can also use all the available features in the dataset.\n",
    "\n",
    "Train a machine learning algorithm: Train a machine learning algorithm using the selected features and evaluate its performance using a suitable metric, such as mean squared error or R-squared.\n",
    "\n",
    "Perform feature selection: Perform feature selection by recursively adding or removing features from the subset and retraining the machine learning algorithm. This process continues until the optimal set of features is found.\n",
    "\n",
    "Choose a stopping criterion: Choose a stopping criterion, such as a maximum number of iterations or a minimum improvement in performance, to avoid overfitting or underfitting.\n",
    "\n",
    "Evaluate the model: Once the optimal set of features is found, evaluate the model's performance using cross-validation or a separate validation dataset to estimate its generalization ability.\n",
    "\n",
    "In summary, to use the Wrapper method to select the best set of features for predicting the price of a house, you need to start with a subset of features, train a machine learning algorithm, perform feature selection by recursively adding or removing features, choose a stopping criterion, and evaluate the model's performance. The Wrapper method is computationally expensive and may suffer from overfitting, but it can potentially identify the best set of features for the predictor.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a984bb1-bc51-4680-9fc0-6f3721709258",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
