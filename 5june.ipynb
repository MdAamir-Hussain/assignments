{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e50cdcd-7db8-48e6-8289-9c5b36f36815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the context of artificial neural networks, an activation function is a crucial component of individual neurons (or nodes) within the neural network. It determines whether the neuron should be activated (or \"fired\") and pass its output to the next layer in the network based on the input it receives.\\n\\nWhen a neural network is designed, it is composed of layers of interconnected nodes. Each node receives input from the previous layer, processes it using an activation function, and then produces an output that is passed on to the next layer. The activation function introduces non-linearity to the neural network, allowing it to learn complex patterns and make predictions on a wide range of problems.\\n\\nThe activation function takes the weighted sum of the inputs (often with an added bias term) and applies a mathematical function to it. The output of this function becomes the output of the neuron and may be used as input for subsequent layers.\\n\\nThere are several types of activation functions used in neural networks, and some popular ones include:\\n\\n1. **Sigmoid function**: It squashes the input values between 0 and 1, which is useful in binary classification problems. However, it suffers from the vanishing gradient problem, which can slow down learning in deep networks.\\n\\n2. **ReLU (Rectified Linear Unit)**: It returns the input as the output if it is positive, and zero otherwise. ReLU has become widely popular due to its simplicity and ability to alleviate the vanishing gradient problem.\\n\\n3. **Leaky ReLU**: It is similar to ReLU but allows a small, non-zero gradient when the input is negative. This helps with the \"dying ReLU\" problem, where neurons can get stuck during training and never activate again.\\n\\n4. **Hyperbolic tangent (tanh)**: Similar to the sigmoid function but maps the inputs to the range [-1, 1], centered around zero.\\n\\n5. **Softmax**: Used primarily in the output layer for multi-class classification problems, softmax converts a vector of real values into a probability distribution, with each value representing the probability of the input belonging to a specific class.\\n\\nDifferent activation functions are chosen based on the requirements of the task, the architecture of the neural network, and the characteristics of the data being processed. The choice of activation function can impact the network\\'s convergence speed, performance, and generalization ability.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1\n",
    "'''In the context of artificial neural networks, an activation function is a crucial component of individual neurons (or nodes) within the neural network. It determines whether the neuron should be activated (or \"fired\") and pass its output to the next layer in the network based on the input it receives.\n",
    "\n",
    "When a neural network is designed, it is composed of layers of interconnected nodes. Each node receives input from the previous layer, processes it using an activation function, and then produces an output that is passed on to the next layer. The activation function introduces non-linearity to the neural network, allowing it to learn complex patterns and make predictions on a wide range of problems.\n",
    "\n",
    "The activation function takes the weighted sum of the inputs (often with an added bias term) and applies a mathematical function to it. The output of this function becomes the output of the neuron and may be used as input for subsequent layers.\n",
    "\n",
    "There are several types of activation functions used in neural networks, and some popular ones include:\n",
    "\n",
    "1. **Sigmoid function**: It squashes the input values between 0 and 1, which is useful in binary classification problems. However, it suffers from the vanishing gradient problem, which can slow down learning in deep networks.\n",
    "\n",
    "2. **ReLU (Rectified Linear Unit)**: It returns the input as the output if it is positive, and zero otherwise. ReLU has become widely popular due to its simplicity and ability to alleviate the vanishing gradient problem.\n",
    "\n",
    "3. **Leaky ReLU**: It is similar to ReLU but allows a small, non-zero gradient when the input is negative. This helps with the \"dying ReLU\" problem, where neurons can get stuck during training and never activate again.\n",
    "\n",
    "4. **Hyperbolic tangent (tanh)**: Similar to the sigmoid function but maps the inputs to the range [-1, 1], centered around zero.\n",
    "\n",
    "5. **Softmax**: Used primarily in the output layer for multi-class classification problems, softmax converts a vector of real values into a probability distribution, with each value representing the probability of the input belonging to a specific class.\n",
    "\n",
    "Different activation functions are chosen based on the requirements of the task, the architecture of the neural network, and the characteristics of the data being processed. The choice of activation function can impact the network's convergence speed, performance, and generalization ability.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddcf6a40-5473-4560-a28f-ca26be0e1834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As mentioned earlier, there are several common types of activation functions used in neural networks. Here\\'s a summary of some popular ones:\\n\\n1. **Sigmoid function (Logistic Activation)**:\\n   - Range: Output values between 0 and 1.\\n   - Formula: f(x) = 1 / (1 + exp(-x)).\\n   - Use case: Historically used in the output layer for binary classification problems. However, it is less commonly used in hidden layers nowadays due to the vanishing gradient problem.\\n\\n2. **ReLU (Rectified Linear Unit)**:\\n   - Range: Output values are zero for negative inputs, and the input value for positive inputs.\\n   - Formula: f(x) = max(0, x).\\n   - Use case: Widely used in hidden layers of deep neural networks due to its simplicity and ability to alleviate the vanishing gradient problem.\\n\\n3. **Leaky ReLU**:\\n   - Range: Output values are zero for negative inputs but allow a small, non-zero gradient for negative inputs.\\n   - Formula: f(x) = max(ax, x), where \\'a\\' is a small positive slope.\\n   - Use case: Addresses the \"dying ReLU\" problem, where neurons can become inactive during training and never activate again.\\n\\n4. **Parametric ReLU (PReLU)**:\\n   - Range: Output values are zero for negative inputs but allow a learnable parameter for negative inputs.\\n   - Formula: f(x) = max(ax, x), where \\'a\\' is a learnable parameter during training.\\n   - Use case: Similar to Leaky ReLU, it addresses the \"dying ReLU\" problem but allows the slope to be learned.\\n\\n5. **Exponential Linear Unit (ELU)**:\\n   - Range: Output values are close to zero for negative inputs and exponential for positive inputs.\\n   - Formula: f(x) = x if x > 0, and f(x) = a * (exp(x) - 1) if x <= 0, where \\'a\\' is a hyperparameter.\\n   - Use case: ELU can provide smoother gradients compared to ReLU and is useful when faster convergence is desired.\\n\\n6. **Hyperbolic tangent (tanh)**:\\n   - Range: Output values between -1 and 1.\\n   - Formula: f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x)).\\n   - Use case: Similar to sigmoid, tanh is used in situations where the output range needs to be centered around zero.\\n\\n7. **Softmax**:\\n   - Range: Transforms a vector of real values into a probability distribution.\\n   - Formula: f(x_i) = exp(x_i) / sum(exp(x_j)), where the sum is over all elements in the input vector.\\n   - Use case: Typically used in the output layer for multi-class classification problems, as it provides class probabilities that sum to 1.\\n\\nThe choice of activation function depends on the specific neural network architecture, the nature of the problem being solved, and empirical experimentation to determine which activation function performs best for a particular task.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2\n",
    "'''As mentioned earlier, there are several common types of activation functions used in neural networks. Here's a summary of some popular ones:\n",
    "\n",
    "1. **Sigmoid function (Logistic Activation)**:\n",
    "   - Range: Output values between 0 and 1.\n",
    "   - Formula: f(x) = 1 / (1 + exp(-x)).\n",
    "   - Use case: Historically used in the output layer for binary classification problems. However, it is less commonly used in hidden layers nowadays due to the vanishing gradient problem.\n",
    "\n",
    "2. **ReLU (Rectified Linear Unit)**:\n",
    "   - Range: Output values are zero for negative inputs, and the input value for positive inputs.\n",
    "   - Formula: f(x) = max(0, x).\n",
    "   - Use case: Widely used in hidden layers of deep neural networks due to its simplicity and ability to alleviate the vanishing gradient problem.\n",
    "\n",
    "3. **Leaky ReLU**:\n",
    "   - Range: Output values are zero for negative inputs but allow a small, non-zero gradient for negative inputs.\n",
    "   - Formula: f(x) = max(ax, x), where 'a' is a small positive slope.\n",
    "   - Use case: Addresses the \"dying ReLU\" problem, where neurons can become inactive during training and never activate again.\n",
    "\n",
    "4. **Parametric ReLU (PReLU)**:\n",
    "   - Range: Output values are zero for negative inputs but allow a learnable parameter for negative inputs.\n",
    "   - Formula: f(x) = max(ax, x), where 'a' is a learnable parameter during training.\n",
    "   - Use case: Similar to Leaky ReLU, it addresses the \"dying ReLU\" problem but allows the slope to be learned.\n",
    "\n",
    "5. **Exponential Linear Unit (ELU)**:\n",
    "   - Range: Output values are close to zero for negative inputs and exponential for positive inputs.\n",
    "   - Formula: f(x) = x if x > 0, and f(x) = a * (exp(x) - 1) if x <= 0, where 'a' is a hyperparameter.\n",
    "   - Use case: ELU can provide smoother gradients compared to ReLU and is useful when faster convergence is desired.\n",
    "\n",
    "6. **Hyperbolic tangent (tanh)**:\n",
    "   - Range: Output values between -1 and 1.\n",
    "   - Formula: f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x)).\n",
    "   - Use case: Similar to sigmoid, tanh is used in situations where the output range needs to be centered around zero.\n",
    "\n",
    "7. **Softmax**:\n",
    "   - Range: Transforms a vector of real values into a probability distribution.\n",
    "   - Formula: f(x_i) = exp(x_i) / sum(exp(x_j)), where the sum is over all elements in the input vector.\n",
    "   - Use case: Typically used in the output layer for multi-class classification problems, as it provides class probabilities that sum to 1.\n",
    "\n",
    "The choice of activation function depends on the specific neural network architecture, the nature of the problem being solved, and empirical experimentation to determine which activation function performs best for a particular task.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eee1054b-dcdf-4c65-a80f-80efb3f9767c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Activation functions play a crucial role in the training process and performance of a neural network. The choice of activation function can impact how well the network learns, its convergence speed during training, and its overall performance on the task at hand. Here\\'s how activation functions affect neural network training and performance:\\n\\n1. **Non-Linearity**: Activation functions introduce non-linearity to the neural network. Without non-linearity, the neural network would be reduced to a linear model, unable to learn complex patterns and relationships in the data. Non-linear activation functions allow the network to approximate more complex functions, making it capable of solving a wider range of problems.\\n\\n2. **Gradient Flow and Vanishing Gradient Problem**: Activation functions affect the flow of gradients during backpropagation, which is the process of updating the network\\'s weights based on the error between predicted and actual outputs. The choice of activation function can impact how well gradients flow through the network during backpropagation.\\n\\n   - ReLU and its variants (Leaky ReLU, PReLU) tend to have better gradient flow for positive inputs, leading to faster convergence. However, they can suffer from the \"dying ReLU\" problem for negative inputs.\\n   - Sigmoid and tanh activation functions suffer from the vanishing gradient problem, where gradients become very small for extreme input values, leading to slow convergence.\\n\\n3. **Convergence Speed**: Activation functions influence how quickly the neural network converges to a solution during training. Activation functions with better gradient flow and non-saturating behavior (such as ReLU and its variants) can lead to faster convergence compared to saturating activation functions like sigmoid and tanh.\\n\\n4. **Avoiding Saturation**: Activation functions like sigmoid and tanh saturate for extreme input values, causing the neurons to respond very weakly to certain inputs. This can lead to a phenomenon known as the \"vanishing/exploding gradient problem,\" where gradients become too small or too large, hindering the learning process. ReLU and its variants are less prone to saturation and are more stable during training.\\n\\n5. **Generalization**: The choice of activation function can affect the generalization ability of the neural network. Some activation functions might lead to overfitting on the training data, while others might provide better generalization to unseen data. Proper regularization techniques can also help mitigate overfitting.\\n\\n6. **Output Range**: Activation functions can determine the output range of neurons. For example, sigmoid and tanh activations constrain the output within specific ranges (0 to 1 and -1 to 1, respectively), which can be advantageous in certain scenarios. Softmax activation, used in the output layer for classification tasks, ensures the outputs form a valid probability distribution.\\n\\n7. **Hyperparameter Tuning**: In some activation functions (e.g., ELU), there are hyperparameters that need to be chosen. The performance of the network can depend on these hyperparameters, and they may need to be tuned to achieve optimal results.\\n\\nIn summary, activation functions significantly impact the behavior of neural networks during training and influence their ability to learn complex patterns from data. The right choice of activation function is an important design consideration to ensure the neural network performs well on the specific task and data at hand.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3\n",
    "'''Activation functions play a crucial role in the training process and performance of a neural network. The choice of activation function can impact how well the network learns, its convergence speed during training, and its overall performance on the task at hand. Here's how activation functions affect neural network training and performance:\n",
    "\n",
    "1. **Non-Linearity**: Activation functions introduce non-linearity to the neural network. Without non-linearity, the neural network would be reduced to a linear model, unable to learn complex patterns and relationships in the data. Non-linear activation functions allow the network to approximate more complex functions, making it capable of solving a wider range of problems.\n",
    "\n",
    "2. **Gradient Flow and Vanishing Gradient Problem**: Activation functions affect the flow of gradients during backpropagation, which is the process of updating the network's weights based on the error between predicted and actual outputs. The choice of activation function can impact how well gradients flow through the network during backpropagation.\n",
    "\n",
    "   - ReLU and its variants (Leaky ReLU, PReLU) tend to have better gradient flow for positive inputs, leading to faster convergence. However, they can suffer from the \"dying ReLU\" problem for negative inputs.\n",
    "   - Sigmoid and tanh activation functions suffer from the vanishing gradient problem, where gradients become very small for extreme input values, leading to slow convergence.\n",
    "\n",
    "3. **Convergence Speed**: Activation functions influence how quickly the neural network converges to a solution during training. Activation functions with better gradient flow and non-saturating behavior (such as ReLU and its variants) can lead to faster convergence compared to saturating activation functions like sigmoid and tanh.\n",
    "\n",
    "4. **Avoiding Saturation**: Activation functions like sigmoid and tanh saturate for extreme input values, causing the neurons to respond very weakly to certain inputs. This can lead to a phenomenon known as the \"vanishing/exploding gradient problem,\" where gradients become too small or too large, hindering the learning process. ReLU and its variants are less prone to saturation and are more stable during training.\n",
    "\n",
    "5. **Generalization**: The choice of activation function can affect the generalization ability of the neural network. Some activation functions might lead to overfitting on the training data, while others might provide better generalization to unseen data. Proper regularization techniques can also help mitigate overfitting.\n",
    "\n",
    "6. **Output Range**: Activation functions can determine the output range of neurons. For example, sigmoid and tanh activations constrain the output within specific ranges (0 to 1 and -1 to 1, respectively), which can be advantageous in certain scenarios. Softmax activation, used in the output layer for classification tasks, ensures the outputs form a valid probability distribution.\n",
    "\n",
    "7. **Hyperparameter Tuning**: In some activation functions (e.g., ELU), there are hyperparameters that need to be chosen. The performance of the network can depend on these hyperparameters, and they may need to be tuned to achieve optimal results.\n",
    "\n",
    "In summary, activation functions significantly impact the behavior of neural networks during training and influence their ability to learn complex patterns from data. The right choice of activation function is an important design consideration to ensure the neural network performs well on the specific task and data at hand.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f675ca4-a421-4162-ba3e-3a465b2c7af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The sigmoid activation function, also known as the logistic function, is a popular non-linear activation function used in the early days of neural networks. It takes an input and maps it to an output value between 0 and 1. The formula for the sigmoid function is as follows:\\n\\nf(x) = 1 / (1 + exp(-x))\\n\\nwhere 'x' is the input to the function, and 'exp' represents the exponential function.\\n\\n**How the Sigmoid Activation Function Works:**\\nThe sigmoid activation function squashes the input values to a range between 0 and 1. When the input is a large positive number, the function output approaches 1, and when the input is a large negative number, the output approaches 0. At zero input, the output is exactly 0.5.\\n\\n**Advantages of the Sigmoid Activation Function:**\\n1. **Non-linearity**: The sigmoid function introduces non-linearity, allowing neural networks to learn complex patterns and relationships in the data. This is important for handling non-linearly separable data.\\n\\n2. **Output Range**: The output of the sigmoid function is constrained between 0 and 1. This can be useful in situations where the output needs to be interpreted as a probability or used in binary classification problems.\\n\\n3. **Differentiability**: The sigmoid function is differentiable everywhere, which is essential for using gradient-based optimization techniques (like backpropagation) to train neural networks.\\n\\n4. **Historical Relevance**: The sigmoid function was one of the first activation functions used in neural networks and was pivotal in the early development of the field.\\n\\n**Disadvantages of the Sigmoid Activation Function:**\\n1. **Vanishing Gradient**: The sigmoid function suffers from the vanishing gradient problem. For very positive or very negative inputs, the derivative of the sigmoid function approaches zero. This leads to extremely small gradients during backpropagation, which can slow down learning or even stall it altogether, especially in deep networks.\\n\\n2. **Output Saturation**: The sigmoid function saturates for large positive and negative inputs. This means that neurons will respond very weakly to extreme input values, leading to a limited ability to model complex relationships.\\n\\n3. **Not Zero-Centered**: The sigmoid function is not zero-centered, which can cause issues in certain optimization algorithms and slow down convergence.\\n\\n4. **Efficiency**: The sigmoid function requires computing the exponential function, which can be computationally expensive, particularly when applied to large vectors or matrices during forward and backward passes.\\n\\nDue to the disadvantages mentioned above, sigmoid activation functions have been largely replaced by other activation functions like ReLU (Rectified Linear Unit) and its variants, which have proven to be more effective in many deep learning applications. While the sigmoid function is rarely used in hidden layers of modern neural networks, it can still find use in specific situations, such as the output layer of binary classification problems when probability interpretation is desired.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4\n",
    "'''The sigmoid activation function, also known as the logistic function, is a popular non-linear activation function used in the early days of neural networks. It takes an input and maps it to an output value between 0 and 1. The formula for the sigmoid function is as follows:\n",
    "\n",
    "f(x) = 1 / (1 + exp(-x))\n",
    "\n",
    "where 'x' is the input to the function, and 'exp' represents the exponential function.\n",
    "\n",
    "**How the Sigmoid Activation Function Works:**\n",
    "The sigmoid activation function squashes the input values to a range between 0 and 1. When the input is a large positive number, the function output approaches 1, and when the input is a large negative number, the output approaches 0. At zero input, the output is exactly 0.5.\n",
    "\n",
    "**Advantages of the Sigmoid Activation Function:**\n",
    "1. **Non-linearity**: The sigmoid function introduces non-linearity, allowing neural networks to learn complex patterns and relationships in the data. This is important for handling non-linearly separable data.\n",
    "\n",
    "2. **Output Range**: The output of the sigmoid function is constrained between 0 and 1. This can be useful in situations where the output needs to be interpreted as a probability or used in binary classification problems.\n",
    "\n",
    "3. **Differentiability**: The sigmoid function is differentiable everywhere, which is essential for using gradient-based optimization techniques (like backpropagation) to train neural networks.\n",
    "\n",
    "4. **Historical Relevance**: The sigmoid function was one of the first activation functions used in neural networks and was pivotal in the early development of the field.\n",
    "\n",
    "**Disadvantages of the Sigmoid Activation Function:**\n",
    "1. **Vanishing Gradient**: The sigmoid function suffers from the vanishing gradient problem. For very positive or very negative inputs, the derivative of the sigmoid function approaches zero. This leads to extremely small gradients during backpropagation, which can slow down learning or even stall it altogether, especially in deep networks.\n",
    "\n",
    "2. **Output Saturation**: The sigmoid function saturates for large positive and negative inputs. This means that neurons will respond very weakly to extreme input values, leading to a limited ability to model complex relationships.\n",
    "\n",
    "3. **Not Zero-Centered**: The sigmoid function is not zero-centered, which can cause issues in certain optimization algorithms and slow down convergence.\n",
    "\n",
    "4. **Efficiency**: The sigmoid function requires computing the exponential function, which can be computationally expensive, particularly when applied to large vectors or matrices during forward and backward passes.\n",
    "\n",
    "Due to the disadvantages mentioned above, sigmoid activation functions have been largely replaced by other activation functions like ReLU (Rectified Linear Unit) and its variants, which have proven to be more effective in many deep learning applications. While the sigmoid function is rarely used in hidden layers of modern neural networks, it can still find use in specific situations, such as the output layer of binary classification problems when probability interpretation is desired.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2165daf0-7195-4bd3-971e-3e657a0600cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Rectified Linear Unit (ReLU) activation function is a non-linear activation function widely used in modern neural networks, especially in deep learning architectures. It is one of the most popular activation functions due to its simplicity and ability to address some of the limitations of other activation functions like the sigmoid.\\n\\n**ReLU Activation Function:**\\nThe ReLU activation function takes an input \\'x\\' and returns the maximum of 0 and the input value. Mathematically, the ReLU function can be defined as follows:\\n\\nf(x) = max(0, x)\\n\\nIn other words, if the input is positive or zero, the ReLU returns the input value unchanged. If the input is negative, it sets the output to zero. The ReLU function introduces non-linearity, which is essential for the neural network to learn complex patterns in the data.\\n\\n**Differences between ReLU and Sigmoid Activation Functions:**\\n\\n1. **Output Range:**\\n   - Sigmoid Function: The sigmoid activation function maps the input to a value between 0 and 1. This can be useful for problems where the output needs to be interpreted as a probability or for binary classification tasks.\\n   - ReLU Function: The ReLU activation function maps the input to 0 for negative values and keeps the input unchanged for non-negative values. This means that the output range of ReLU is from 0 to positive infinity.\\n\\n2. **Non-linearity:**\\n   - Sigmoid Function: The sigmoid function is non-linear but has a vanishing gradient problem, especially for extreme input values, which can slow down learning in deep networks.\\n   - ReLU Function: The ReLU function is also non-linear but avoids the vanishing gradient problem for positive inputs, leading to faster convergence during training. However, it can suffer from the \"dying ReLU\" problem, where neurons can become inactive (output zero) for negative inputs and never recover.\\n\\n3. **Computation Efficiency:**\\n   - Sigmoid Function: The sigmoid function requires computing the exponential function, which can be computationally expensive, especially when applied to large datasets.\\n   - ReLU Function: The ReLU function is computationally efficient, as it involves simple element-wise operations, making it faster to compute.\\n\\n4. **Centered Around Zero:**\\n   - Sigmoid Function: The sigmoid function is centered around 0.5, which may not always be desirable in some neural network architectures.\\n   - ReLU Function: The ReLU function is not centered around zero, which can be both an advantage and disadvantage depending on the context. This can cause some optimization algorithms to oscillate around zero, but it has been observed that this doesn\\'t significantly impact training performance.\\n\\nIn summary, the ReLU activation function offers several advantages over the sigmoid function, such as avoiding the vanishing gradient problem, computational efficiency, and simple implementation. However, it may suffer from the \"dying ReLU\" problem, especially in deeper networks. Despite this limitation, ReLU and its variants (Leaky ReLU, Parametric ReLU, etc.) are widely used as activation functions in many neural network architectures due to their effectiveness in deep learning tasks.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5\n",
    "'''The Rectified Linear Unit (ReLU) activation function is a non-linear activation function widely used in modern neural networks, especially in deep learning architectures. It is one of the most popular activation functions due to its simplicity and ability to address some of the limitations of other activation functions like the sigmoid.\n",
    "\n",
    "**ReLU Activation Function:**\n",
    "The ReLU activation function takes an input 'x' and returns the maximum of 0 and the input value. Mathematically, the ReLU function can be defined as follows:\n",
    "\n",
    "f(x) = max(0, x)\n",
    "\n",
    "In other words, if the input is positive or zero, the ReLU returns the input value unchanged. If the input is negative, it sets the output to zero. The ReLU function introduces non-linearity, which is essential for the neural network to learn complex patterns in the data.\n",
    "\n",
    "**Differences between ReLU and Sigmoid Activation Functions:**\n",
    "\n",
    "1. **Output Range:**\n",
    "   - Sigmoid Function: The sigmoid activation function maps the input to a value between 0 and 1. This can be useful for problems where the output needs to be interpreted as a probability or for binary classification tasks.\n",
    "   - ReLU Function: The ReLU activation function maps the input to 0 for negative values and keeps the input unchanged for non-negative values. This means that the output range of ReLU is from 0 to positive infinity.\n",
    "\n",
    "2. **Non-linearity:**\n",
    "   - Sigmoid Function: The sigmoid function is non-linear but has a vanishing gradient problem, especially for extreme input values, which can slow down learning in deep networks.\n",
    "   - ReLU Function: The ReLU function is also non-linear but avoids the vanishing gradient problem for positive inputs, leading to faster convergence during training. However, it can suffer from the \"dying ReLU\" problem, where neurons can become inactive (output zero) for negative inputs and never recover.\n",
    "\n",
    "3. **Computation Efficiency:**\n",
    "   - Sigmoid Function: The sigmoid function requires computing the exponential function, which can be computationally expensive, especially when applied to large datasets.\n",
    "   - ReLU Function: The ReLU function is computationally efficient, as it involves simple element-wise operations, making it faster to compute.\n",
    "\n",
    "4. **Centered Around Zero:**\n",
    "   - Sigmoid Function: The sigmoid function is centered around 0.5, which may not always be desirable in some neural network architectures.\n",
    "   - ReLU Function: The ReLU function is not centered around zero, which can be both an advantage and disadvantage depending on the context. This can cause some optimization algorithms to oscillate around zero, but it has been observed that this doesn't significantly impact training performance.\n",
    "\n",
    "In summary, the ReLU activation function offers several advantages over the sigmoid function, such as avoiding the vanishing gradient problem, computational efficiency, and simple implementation. However, it may suffer from the \"dying ReLU\" problem, especially in deeper networks. Despite this limitation, ReLU and its variants (Leaky ReLU, Parametric ReLU, etc.) are widely used as activation functions in many neural network architectures due to their effectiveness in deep learning tasks.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ad5463b-16e3-4a78-8aa6-2d1ed56add08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Using the Rectified Linear Unit (ReLU) activation function over the sigmoid function offers several benefits, which have contributed to its widespread adoption in modern neural networks. Here are some of the key advantages of ReLU:\\n\\n1. **Avoiding the Vanishing Gradient Problem**: One of the significant issues with the sigmoid activation function is the vanishing gradient problem. For extreme input values, the derivative of the sigmoid function approaches zero, leading to very small gradients during backpropagation. This can cause slow learning or even prevent convergence, particularly in deep networks with many layers. ReLU does not suffer from this problem for positive inputs, as its derivative is a constant (1) for positive values, allowing for more efficient gradient flow during training.\\n\\n2. **Computational Efficiency**: ReLU is computationally more efficient compared to the sigmoid function. The sigmoid function requires computing the expensive exponential function, which can be time-consuming, especially when applied to large datasets or deep networks. In contrast, ReLU only involves simple element-wise operations (max function), making it faster to compute.\\n\\n3. **Non-Saturating Behavior**: The sigmoid activation function saturates (approaches 0 or 1) for extreme positive and negative inputs. This saturation limits the ability of neurons to model complex relationships, and the network may struggle to learn when most neurons are stuck in this saturated regime. ReLU does not saturate for positive inputs, allowing neurons to better capture non-linear patterns in the data.\\n\\n4. **Sparsity**: ReLU can lead to sparsity in neural networks. Since ReLU outputs zero for negative inputs, some neurons might become inactive (output zero) for certain data points during training. This sparsity can lead to more efficient network representations, as fewer neurons are involved in the computation, reducing computational overhead.\\n\\n5. **Ease of Implementation**: Implementing ReLU is straightforward and requires minimal computational overhead. The ReLU activation function is a simple element-wise operation, making it easy to incorporate into various neural network architectures and frameworks.\\n\\n6. **Improved Convergence Speed**: Due to its non-saturating and efficient gradient flow properties, ReLU often leads to faster convergence during training compared to the sigmoid function. This is particularly beneficial when training deep neural networks, as faster convergence reduces the time required to find a good solution.\\n\\n7. **Initialization**: ReLU activations can be more favorable for weight initialization techniques, such as He initialization, which works well with ReLU activations and contributes to more stable training.\\n\\nDespite these benefits, it\\'s essential to acknowledge that ReLU is not without its challenges. The \"dying ReLU\" problem can occur, where neurons can become inactive (output zero) for negative inputs and never recover. This can lead to dead neurons that do not contribute to the learning process. To address this, various variants of ReLU have been proposed, such as Leaky ReLU, Parametric ReLU, and Exponential Linear Unit (ELU), which aim to mitigate the dying ReLU problem and provide improved performance in specific scenarios.\\n\\nOverall, the advantages of ReLU make it a popular choice for the activation function in modern neural network architectures, especially in deep learning applications.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#6\n",
    "'''Using the Rectified Linear Unit (ReLU) activation function over the sigmoid function offers several benefits, which have contributed to its widespread adoption in modern neural networks. Here are some of the key advantages of ReLU:\n",
    "\n",
    "1. **Avoiding the Vanishing Gradient Problem**: One of the significant issues with the sigmoid activation function is the vanishing gradient problem. For extreme input values, the derivative of the sigmoid function approaches zero, leading to very small gradients during backpropagation. This can cause slow learning or even prevent convergence, particularly in deep networks with many layers. ReLU does not suffer from this problem for positive inputs, as its derivative is a constant (1) for positive values, allowing for more efficient gradient flow during training.\n",
    "\n",
    "2. **Computational Efficiency**: ReLU is computationally more efficient compared to the sigmoid function. The sigmoid function requires computing the expensive exponential function, which can be time-consuming, especially when applied to large datasets or deep networks. In contrast, ReLU only involves simple element-wise operations (max function), making it faster to compute.\n",
    "\n",
    "3. **Non-Saturating Behavior**: The sigmoid activation function saturates (approaches 0 or 1) for extreme positive and negative inputs. This saturation limits the ability of neurons to model complex relationships, and the network may struggle to learn when most neurons are stuck in this saturated regime. ReLU does not saturate for positive inputs, allowing neurons to better capture non-linear patterns in the data.\n",
    "\n",
    "4. **Sparsity**: ReLU can lead to sparsity in neural networks. Since ReLU outputs zero for negative inputs, some neurons might become inactive (output zero) for certain data points during training. This sparsity can lead to more efficient network representations, as fewer neurons are involved in the computation, reducing computational overhead.\n",
    "\n",
    "5. **Ease of Implementation**: Implementing ReLU is straightforward and requires minimal computational overhead. The ReLU activation function is a simple element-wise operation, making it easy to incorporate into various neural network architectures and frameworks.\n",
    "\n",
    "6. **Improved Convergence Speed**: Due to its non-saturating and efficient gradient flow properties, ReLU often leads to faster convergence during training compared to the sigmoid function. This is particularly beneficial when training deep neural networks, as faster convergence reduces the time required to find a good solution.\n",
    "\n",
    "7. **Initialization**: ReLU activations can be more favorable for weight initialization techniques, such as He initialization, which works well with ReLU activations and contributes to more stable training.\n",
    "\n",
    "Despite these benefits, it's essential to acknowledge that ReLU is not without its challenges. The \"dying ReLU\" problem can occur, where neurons can become inactive (output zero) for negative inputs and never recover. This can lead to dead neurons that do not contribute to the learning process. To address this, various variants of ReLU have been proposed, such as Leaky ReLU, Parametric ReLU, and Exponential Linear Unit (ELU), which aim to mitigate the dying ReLU problem and provide improved performance in specific scenarios.\n",
    "\n",
    "Overall, the advantages of ReLU make it a popular choice for the activation function in modern neural network architectures, especially in deep learning applications.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4fbafd03-4d20-42e8-bb2d-d42816018d31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Leaky ReLU is a variation of the Rectified Linear Unit (ReLU) activation function. While ReLU sets negative input values to zero, Leaky ReLU introduces a small, non-zero slope for negative inputs, allowing the activation function to have a non-zero output for negative values. The mathematical expression for the Leaky ReLU activation function is as follows:\\n\\nf(x) = max(ax, x)\\n\\nwhere \\'x\\' is the input to the function, and \\'a\\' is a hyperparameter that determines the slope for negative inputs. Typically, \\'a\\' is set to a small positive value, such as 0.01.\\n\\n**Addressing the Vanishing Gradient Problem:**\\nThe vanishing gradient problem is a common issue that can occur during the training of deep neural networks, particularly when using activation functions like sigmoid or tanh. As the gradients propagate back through the layers during backpropagation, they can become very small for certain activations, causing the weights to update negligibly or even stagnate, leading to slow or stalled learning.\\n\\nLeaky ReLU addresses the vanishing gradient problem by providing a non-zero slope for negative inputs. This means that even when the input is negative, Leaky ReLU allows a small, non-zero gradient to pass through during backpropagation. As a result, the gradients do not vanish, and the network can continue learning from the errors and adjusting its weights more effectively, especially in deep networks.\\n\\n**Advantages of Leaky ReLU:**\\n1. **Mitigating the \"Dying ReLU\" Problem**: In standard ReLU, some neurons can become inactive (output zero) for negative inputs and never recover, leading to dead neurons and reduced model capacity. Leaky ReLU addresses this issue by allowing small negative values to flow through, preventing neurons from dying completely.\\n\\n2. **Preventing Gradient Saturation**: Leaky ReLU can prevent the saturation of gradients for negative inputs, which can occur with activation functions like sigmoid and tanh. This enables more stable and faster training in deeper networks.\\n\\n3. **Improved Learning**: By providing a non-zero slope for negative inputs, Leaky ReLU can promote learning in cases where the standard ReLU might have saturated or become inactive.\\n\\n4. **Simple Implementation**: Leaky ReLU is simple to implement and does not require additional computational overhead compared to standard ReLU.\\n\\n**Choosing the Hyperparameter \\'a\\':**\\nThe value of the hyperparameter \\'a\\' in Leaky ReLU is usually set to a small positive constant, such as 0.01. However, this value can be adjusted based on the problem and dataset. In practice, the value of \\'a\\' can also be treated as a hyperparameter and tuned during the training process to find the best value for a specific task.\\n\\nWhile Leaky ReLU has proven to be effective in some cases, it is worth noting that it is not a one-size-fits-all solution. Different activation functions, including Leaky ReLU, ReLU, and others like ELU (Exponential Linear Unit), have their strengths and weaknesses and may perform differently based on the task and dataset. As a result, it is essential to experiment with various activation functions to determine the best fit for a particular neural network architecture and problem domain.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#7\n",
    "'''Leaky ReLU is a variation of the Rectified Linear Unit (ReLU) activation function. While ReLU sets negative input values to zero, Leaky ReLU introduces a small, non-zero slope for negative inputs, allowing the activation function to have a non-zero output for negative values. The mathematical expression for the Leaky ReLU activation function is as follows:\n",
    "\n",
    "f(x) = max(ax, x)\n",
    "\n",
    "where 'x' is the input to the function, and 'a' is a hyperparameter that determines the slope for negative inputs. Typically, 'a' is set to a small positive value, such as 0.01.\n",
    "\n",
    "**Addressing the Vanishing Gradient Problem:**\n",
    "The vanishing gradient problem is a common issue that can occur during the training of deep neural networks, particularly when using activation functions like sigmoid or tanh. As the gradients propagate back through the layers during backpropagation, they can become very small for certain activations, causing the weights to update negligibly or even stagnate, leading to slow or stalled learning.\n",
    "\n",
    "Leaky ReLU addresses the vanishing gradient problem by providing a non-zero slope for negative inputs. This means that even when the input is negative, Leaky ReLU allows a small, non-zero gradient to pass through during backpropagation. As a result, the gradients do not vanish, and the network can continue learning from the errors and adjusting its weights more effectively, especially in deep networks.\n",
    "\n",
    "**Advantages of Leaky ReLU:**\n",
    "1. **Mitigating the \"Dying ReLU\" Problem**: In standard ReLU, some neurons can become inactive (output zero) for negative inputs and never recover, leading to dead neurons and reduced model capacity. Leaky ReLU addresses this issue by allowing small negative values to flow through, preventing neurons from dying completely.\n",
    "\n",
    "2. **Preventing Gradient Saturation**: Leaky ReLU can prevent the saturation of gradients for negative inputs, which can occur with activation functions like sigmoid and tanh. This enables more stable and faster training in deeper networks.\n",
    "\n",
    "3. **Improved Learning**: By providing a non-zero slope for negative inputs, Leaky ReLU can promote learning in cases where the standard ReLU might have saturated or become inactive.\n",
    "\n",
    "4. **Simple Implementation**: Leaky ReLU is simple to implement and does not require additional computational overhead compared to standard ReLU.\n",
    "\n",
    "**Choosing the Hyperparameter 'a':**\n",
    "The value of the hyperparameter 'a' in Leaky ReLU is usually set to a small positive constant, such as 0.01. However, this value can be adjusted based on the problem and dataset. In practice, the value of 'a' can also be treated as a hyperparameter and tuned during the training process to find the best value for a specific task.\n",
    "\n",
    "While Leaky ReLU has proven to be effective in some cases, it is worth noting that it is not a one-size-fits-all solution. Different activation functions, including Leaky ReLU, ReLU, and others like ELU (Exponential Linear Unit), have their strengths and weaknesses and may perform differently based on the task and dataset. As a result, it is essential to experiment with various activation functions to determine the best fit for a particular neural network architecture and problem domain.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db11fbf2-91c3-4733-8be0-840c62537a6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The softmax activation function is primarily used in the output layer of a neural network, especially in multi-class classification tasks. Its main purpose is to transform the raw output scores (logits) of the previous layer into a probability distribution over multiple classes. In essence, softmax converts the logits into a set of probabilities, where each probability represents the likelihood of the input belonging to a specific class.\\n\\nThe softmax function is defined as follows for a vector of logits (z_1, z_2, ..., z_k):\\n\\nsoftmax(z_i) = exp(z_i) / sum(exp(z_j)) for i = 1 to k\\n\\nwhere 'k' is the number of classes, 'z_i' is the logit for class 'i', and 'exp' represents the exponential function.\\n\\n**Purpose of Softmax Activation:**\\nThe primary purpose of the softmax activation function is to provide a probabilistic interpretation of the output of the neural network. By converting logits into probabilities, the softmax function allows us to understand the model's confidence in its predictions for each class. The class with the highest probability is considered the predicted class for the input.\\n\\n**Common Use Cases:**\\nSoftmax activation is commonly used in multi-class classification problems, where the objective is to classify an input into one of several mutually exclusive classes. Some examples of common use cases include:\\n\\n1. **Image Classification**: Given an image, the task is to predict which object or class the image belongs to among multiple possible classes (e.g., cat, dog, car, etc.).\\n\\n2. **Natural Language Processing (NLP)**: In NLP tasks like sentiment analysis or language translation, softmax is used to predict the probability distribution over different classes or words.\\n\\n3. **Speech Recognition**: In speech recognition tasks, the softmax function can be used to convert acoustic features into probabilities over possible phonemes or words.\\n\\n**Note**: It is crucial to pair the softmax activation function with an appropriate loss function for training. The most common choice for multi-class classification is the categorical cross-entropy loss, which compares the predicted probabilities with the true labels to compute the model's error. This combination of softmax activation and categorical cross-entropy loss forms a widely used approach for training and optimizing neural networks for multi-class classification tasks.\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#8\n",
    "'''The softmax activation function is primarily used in the output layer of a neural network, especially in multi-class classification tasks. Its main purpose is to transform the raw output scores (logits) of the previous layer into a probability distribution over multiple classes. In essence, softmax converts the logits into a set of probabilities, where each probability represents the likelihood of the input belonging to a specific class.\n",
    "\n",
    "The softmax function is defined as follows for a vector of logits (z_1, z_2, ..., z_k):\n",
    "\n",
    "softmax(z_i) = exp(z_i) / sum(exp(z_j)) for i = 1 to k\n",
    "\n",
    "where 'k' is the number of classes, 'z_i' is the logit for class 'i', and 'exp' represents the exponential function.\n",
    "\n",
    "**Purpose of Softmax Activation:**\n",
    "The primary purpose of the softmax activation function is to provide a probabilistic interpretation of the output of the neural network. By converting logits into probabilities, the softmax function allows us to understand the model's confidence in its predictions for each class. The class with the highest probability is considered the predicted class for the input.\n",
    "\n",
    "**Common Use Cases:**\n",
    "Softmax activation is commonly used in multi-class classification problems, where the objective is to classify an input into one of several mutually exclusive classes. Some examples of common use cases include:\n",
    "\n",
    "1. **Image Classification**: Given an image, the task is to predict which object or class the image belongs to among multiple possible classes (e.g., cat, dog, car, etc.).\n",
    "\n",
    "2. **Natural Language Processing (NLP)**: In NLP tasks like sentiment analysis or language translation, softmax is used to predict the probability distribution over different classes or words.\n",
    "\n",
    "3. **Speech Recognition**: In speech recognition tasks, the softmax function can be used to convert acoustic features into probabilities over possible phonemes or words.\n",
    "\n",
    "**Note**: It is crucial to pair the softmax activation function with an appropriate loss function for training. The most common choice for multi-class classification is the categorical cross-entropy loss, which compares the predicted probabilities with the true labels to compute the model's error. This combination of softmax activation and categorical cross-entropy loss forms a widely used approach for training and optimizing neural networks for multi-class classification tasks.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "112f8aa5-83dc-402e-be5b-f2ae7c9014a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The hyperbolic tangent (tanh) activation function is another type of non-linear activation function used in neural networks. It is similar to the sigmoid function in some ways but maps the input values to a range between -1 and 1, centered around zero. The formula for the tanh activation function is as follows:\\n\\ntanh(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))\\n\\nwhere 'x' is the input to the function, and 'exp' represents the exponential function.\\n\\n**Comparison to the Sigmoid Function:**\\n\\n1. **Output Range:**\\n   - Sigmoid Function: The sigmoid activation function maps the input to a value between 0 and 1.\\n   - Tanh Function: The tanh activation function maps the input to a value between -1 and 1, with an output range centered around zero.\\n\\n2. **Symmetry and Zero-Centered:**\\n   - Sigmoid Function: The sigmoid function is not zero-centered, and its output range is asymmetric around zero (ranging from 0 to 1).\\n   - Tanh Function: The tanh function is zero-centered, meaning its output has a mean of zero. This makes the tanh activation often preferred over sigmoid in some cases because it can have more favorable optimization properties.\\n\\n3. **Gradient Magnitude:**\\n   - Sigmoid Function: The gradient magnitude of the sigmoid function is generally smaller than the gradient magnitude of the tanh function. This can lead to slower convergence in some cases, especially in deep networks with many layers.\\n   - Tanh Function: The gradient magnitude of the tanh function is larger than that of the sigmoid function. This can lead to faster convergence during training, especially in the presence of deeper networks.\\n\\n4. **Non-Linearity and Vanishing Gradient:**\\n   - Both sigmoid and tanh are non-linear activation functions. However, they both suffer from the vanishing gradient problem for extreme input values, particularly for the tanh function, where the gradients approach zero more rapidly. This can lead to slower training in deep neural networks.\\n\\n**When to Use Tanh Activation:**\\nThe tanh activation function is often used in situations where the output range needs to be centered around zero, as it has a mean value of zero. This can be advantageous for certain optimization algorithms and can help prevent certain convergence issues that may occur with other activation functions like sigmoid.\\n\\nHowever, similar to the sigmoid function, tanh can still suffer from the vanishing gradient problem, especially for deep networks. As a result, other activation functions like ReLU and its variants (Leaky ReLU, Parametric ReLU, etc.) are often preferred in modern deep learning architectures due to their ability to mitigate the vanishing gradient problem and provide faster convergence. Nonetheless, tanh can still find use in specific cases, such as in the hidden layers of shallow networks or certain recurrent neural network (RNN) architectures.\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#9\n",
    "'''The hyperbolic tangent (tanh) activation function is another type of non-linear activation function used in neural networks. It is similar to the sigmoid function in some ways but maps the input values to a range between -1 and 1, centered around zero. The formula for the tanh activation function is as follows:\n",
    "\n",
    "tanh(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))\n",
    "\n",
    "where 'x' is the input to the function, and 'exp' represents the exponential function.\n",
    "\n",
    "**Comparison to the Sigmoid Function:**\n",
    "\n",
    "1. **Output Range:**\n",
    "   - Sigmoid Function: The sigmoid activation function maps the input to a value between 0 and 1.\n",
    "   - Tanh Function: The tanh activation function maps the input to a value between -1 and 1, with an output range centered around zero.\n",
    "\n",
    "2. **Symmetry and Zero-Centered:**\n",
    "   - Sigmoid Function: The sigmoid function is not zero-centered, and its output range is asymmetric around zero (ranging from 0 to 1).\n",
    "   - Tanh Function: The tanh function is zero-centered, meaning its output has a mean of zero. This makes the tanh activation often preferred over sigmoid in some cases because it can have more favorable optimization properties.\n",
    "\n",
    "3. **Gradient Magnitude:**\n",
    "   - Sigmoid Function: The gradient magnitude of the sigmoid function is generally smaller than the gradient magnitude of the tanh function. This can lead to slower convergence in some cases, especially in deep networks with many layers.\n",
    "   - Tanh Function: The gradient magnitude of the tanh function is larger than that of the sigmoid function. This can lead to faster convergence during training, especially in the presence of deeper networks.\n",
    "\n",
    "4. **Non-Linearity and Vanishing Gradient:**\n",
    "   - Both sigmoid and tanh are non-linear activation functions. However, they both suffer from the vanishing gradient problem for extreme input values, particularly for the tanh function, where the gradients approach zero more rapidly. This can lead to slower training in deep neural networks.\n",
    "\n",
    "**When to Use Tanh Activation:**\n",
    "The tanh activation function is often used in situations where the output range needs to be centered around zero, as it has a mean value of zero. This can be advantageous for certain optimization algorithms and can help prevent certain convergence issues that may occur with other activation functions like sigmoid.\n",
    "\n",
    "However, similar to the sigmoid function, tanh can still suffer from the vanishing gradient problem, especially for deep networks. As a result, other activation functions like ReLU and its variants (Leaky ReLU, Parametric ReLU, etc.) are often preferred in modern deep learning architectures due to their ability to mitigate the vanishing gradient problem and provide faster convergence. Nonetheless, tanh can still find use in specific cases, such as in the hidden layers of shallow networks or certain recurrent neural network (RNN) architectures.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299413df-971d-456d-b3ec-331f32f1c95c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
