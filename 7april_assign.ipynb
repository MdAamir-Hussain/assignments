{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62a7d473-99f7-44c0-9481-61e722b16865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Polynomial functions and kernel functions are both mathematical constructs used in machine learning algorithms, particularly in the context of support vector machines (SVMs) and kernel methods.\\n\\nPolynomial functions are a type of mathematical function that involves variables raised to non-negative integer powers. In the context of machine learning, polynomial functions are often used to define the decision boundaries or hyperplanes in SVMs. Specifically, polynomial kernels are used to implicitly transform the input data into a higher-dimensional feature space, where a linear classifier can be used to separate the classes. The polynomial kernel computes the similarity or inner product between pairs of input samples in this higher-dimensional space, without explicitly computing the transformation. This allows SVMs to efficiently handle non-linearly separable data by effectively mapping it to a higher-dimensional space.\\n\\nKernel functions, on the other hand, are more general mathematical functions that compute the similarity between pairs of input samples. They are used in kernel methods, a class of algorithms that make use of the kernel trick. The kernel trick allows algorithms to operate in the input space implicitly transformed by a kernel function, without explicitly computing the transformation. This avoids the computational burden associated with working in high-dimensional feature spaces. Kernel functions can be chosen based on the specific properties of the data and the problem at hand. Polynomial kernels are one type of kernel function commonly used in kernel methods.\\n\\nIn summary, polynomial functions are a specific type of function used to define decision boundaries in SVMs, while kernel functions are more general mathematical functions used in kernel methods to compute similarities between input samples. Polynomial kernels are a type of kernel function that applies the polynomial function implicitly to transform the data into a higher-dimensional space.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1.\n",
    "\n",
    "'''Polynomial functions and kernel functions are both mathematical constructs used in machine learning algorithms, particularly in the context of support vector machines (SVMs) and kernel methods.\n",
    "\n",
    "Polynomial functions are a type of mathematical function that involves variables raised to non-negative integer powers. In the context of machine learning, polynomial functions are often used to define the decision boundaries or hyperplanes in SVMs. Specifically, polynomial kernels are used to implicitly transform the input data into a higher-dimensional feature space, where a linear classifier can be used to separate the classes. The polynomial kernel computes the similarity or inner product between pairs of input samples in this higher-dimensional space, without explicitly computing the transformation. This allows SVMs to efficiently handle non-linearly separable data by effectively mapping it to a higher-dimensional space.\n",
    "\n",
    "Kernel functions, on the other hand, are more general mathematical functions that compute the similarity between pairs of input samples. They are used in kernel methods, a class of algorithms that make use of the kernel trick. The kernel trick allows algorithms to operate in the input space implicitly transformed by a kernel function, without explicitly computing the transformation. This avoids the computational burden associated with working in high-dimensional feature spaces. Kernel functions can be chosen based on the specific properties of the data and the problem at hand. Polynomial kernels are one type of kernel function commonly used in kernel methods.\n",
    "\n",
    "In summary, polynomial functions are a specific type of function used to define decision boundaries in SVMs, while kernel functions are more general mathematical functions used in kernel methods to compute similarities between input samples. Polynomial kernels are a type of kernel function that applies the polynomial function implicitly to transform the data into a higher-dimensional space.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a66df58-7f23-4a91-b5c7-39953374737a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "774b1d2e-b72f-4a16-a8b5-69b9c67aeea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (150, 4)\n",
      "Shape of y: (150,)\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Prepare your data\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Get the input features (X) and corresponding labels (y)\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Optional: Perform any necessary data preprocessing steps\n",
    "# For the Iris dataset, preprocessing may not be necessary as it is already clean and standardized.\n",
    "\n",
    "# Print the shape of X and y to verify the data\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08163c3b-ab7b-4758-ba26-37ace0b08a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee168c8c-aa79-4856-ad65-eacc021a150f",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(kernel='poly', degree=3)\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3d5d85b-474d-4ec6-83ff-9eb2241d4b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78c2cfa9-63f3-4c98-996a-44f12883e58f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In Support Vector Regression (SVR), the epsilon parameter determines the width of the epsilon-insensitive tube around the predicted values. It controls the tolerance for errors in the training data, where data points within the tube are considered to have zero error and are not treated as support vectors.\\n\\nIncreasing the value of epsilon in SVR typically leads to an increase in the number of support vectors. Here's why:\\n\\nLarger Epsilon: When epsilon is set to a larger value, the width of the epsilon-insensitive tube increases. This means that data points within a wider margin from the predicted values are considered to have zero error and do not contribute to the determination of support vectors.\\n\\nMore Tolerant to Errors: With a larger epsilon, SVR becomes more tolerant to errors in the training data. This allows for a larger number of training points to be within the epsilon-insensitive tube without being penalized as errors.\\n\\nMore Support Vectors: Support vectors are the data points that lie either on the margin or within the epsilon-insensitive tube. When epsilon is increased, the margin and the tube widen, allowing more data points to fall within this region. As a result, a larger number of points become support vectors.\\n\\nIt's important to note that the choice of epsilon depends on the problem and the desired trade-off between model complexity and accuracy. A smaller epsilon makes the model less tolerant to errors and may result in a smaller number of support vectors, while a larger epsilon makes the model more tolerant to errors and may increase the number of support vectors.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3.\n",
    "\n",
    "'''In Support Vector Regression (SVR), the epsilon parameter determines the width of the epsilon-insensitive tube around the predicted values. It controls the tolerance for errors in the training data, where data points within the tube are considered to have zero error and are not treated as support vectors.\n",
    "\n",
    "Increasing the value of epsilon in SVR typically leads to an increase in the number of support vectors. Here's why:\n",
    "\n",
    "Larger Epsilon: When epsilon is set to a larger value, the width of the epsilon-insensitive tube increases. This means that data points within a wider margin from the predicted values are considered to have zero error and do not contribute to the determination of support vectors.\n",
    "\n",
    "More Tolerant to Errors: With a larger epsilon, SVR becomes more tolerant to errors in the training data. This allows for a larger number of training points to be within the epsilon-insensitive tube without being penalized as errors.\n",
    "\n",
    "More Support Vectors: Support vectors are the data points that lie either on the margin or within the epsilon-insensitive tube. When epsilon is increased, the margin and the tube widen, allowing more data points to fall within this region. As a result, a larger number of points become support vectors.\n",
    "\n",
    "It's important to note that the choice of epsilon depends on the problem and the desired trade-off between model complexity and accuracy. A smaller epsilon makes the model less tolerant to errors and may result in a smaller number of support vectors, while a larger epsilon makes the model more tolerant to errors and may increase the number of support vectors.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec4ddf04-33ec-4347-b822-bb58769205a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The performance of Support Vector Regression (SVR) is influenced by several parameters: the choice of kernel function, C parameter, epsilon parameter, and gamma parameter. Let's understand each parameter and its impact on SVR performance:\\n\\nKernel Function:\\nThe kernel function determines the type of non-linear mapping applied to the input data. Common kernel functions in SVR include linear, polynomial, radial basis function (RBF), and sigmoid. The choice of the kernel function depends on the underlying patterns in the data. Here are some scenarios where specific kernel functions can be useful:\\nLinear: Use when the data has a linear relationship and you want a simpler model.\\nPolynomial: Use when the data exhibits polynomial patterns.\\nRBF: Use when the data has complex non-linear relationships or when you want a more flexible model that can capture intricate patterns.\\nSigmoid: Use when the data exhibits sigmoidal relationships.\\nC Parameter:\\nThe C parameter controls the trade-off between the model's simplicity (smoothness) and its ability to fit the training data (accuracy). Higher values of C allow the model to fit the training data more closely, potentially resulting in overfitting, while smaller values of C enforce a smoother model. Consider the following:\\nIncrease C: When you have a small dataset or when you suspect the data contains significant noise, increasing C can help the model fit the training data more precisely.\\nDecrease C: When you have a large dataset or want to prevent overfitting, decreasing C can encourage a smoother and more generalized model.\\nEpsilon Parameter:\\nThe epsilon parameter determines the width of the epsilon-insensitive tube around the predicted values in SVR. It defines the tolerance for errors within which data points are considered to have zero error and are not treated as support vectors. Here's how epsilon impacts the SVR model:\\nIncrease Epsilon: A larger epsilon makes the model more tolerant to errors, allowing more data points to fall within the epsilon-insensitive tube. This can lead to a larger number of support vectors and a more flexible model.\\nDecrease Epsilon: A smaller epsilon makes the model less tolerant to errors, resulting in a narrower epsilon-insensitive tube. This can lead to a smaller number of support vectors and a more constrained model.\\nGamma Parameter:\\nThe gamma parameter controls the influence of each training example in the SVR model. It determines the shape of the RBF kernel and affects the flexibility of the model. High gamma values lead to more complex decision boundaries, while low gamma values result in smoother decision boundaries. Consider the following:\\nIncrease Gamma: When the data is highly nonlinear or when you want the model to focus more on nearby points, increasing gamma can result in more intricate decision boundaries and potentially lead to overfitting.\\nDecrease Gamma: When the data is relatively linear or when you want a smoother decision boundary, decreasing gamma can make the model generalize better.\\nIt's important to note that the optimal values for these parameters depend on the specific dataset and problem at hand. It is recommended to perform parameter tuning using techniques such as cross-validation to find the best combination of values that yields the highest performance on unseen data.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4.\n",
    "\n",
    "'''The performance of Support Vector Regression (SVR) is influenced by several parameters: the choice of kernel function, C parameter, epsilon parameter, and gamma parameter. Let's understand each parameter and its impact on SVR performance:\n",
    "\n",
    "Kernel Function:\n",
    "The kernel function determines the type of non-linear mapping applied to the input data. Common kernel functions in SVR include linear, polynomial, radial basis function (RBF), and sigmoid. The choice of the kernel function depends on the underlying patterns in the data. Here are some scenarios where specific kernel functions can be useful:\n",
    "Linear: Use when the data has a linear relationship and you want a simpler model.\n",
    "Polynomial: Use when the data exhibits polynomial patterns.\n",
    "RBF: Use when the data has complex non-linear relationships or when you want a more flexible model that can capture intricate patterns.\n",
    "Sigmoid: Use when the data exhibits sigmoidal relationships.\n",
    "C Parameter:\n",
    "The C parameter controls the trade-off between the model's simplicity (smoothness) and its ability to fit the training data (accuracy). Higher values of C allow the model to fit the training data more closely, potentially resulting in overfitting, while smaller values of C enforce a smoother model. Consider the following:\n",
    "Increase C: When you have a small dataset or when you suspect the data contains significant noise, increasing C can help the model fit the training data more precisely.\n",
    "Decrease C: When you have a large dataset or want to prevent overfitting, decreasing C can encourage a smoother and more generalized model.\n",
    "Epsilon Parameter:\n",
    "The epsilon parameter determines the width of the epsilon-insensitive tube around the predicted values in SVR. It defines the tolerance for errors within which data points are considered to have zero error and are not treated as support vectors. Here's how epsilon impacts the SVR model:\n",
    "Increase Epsilon: A larger epsilon makes the model more tolerant to errors, allowing more data points to fall within the epsilon-insensitive tube. This can lead to a larger number of support vectors and a more flexible model.\n",
    "Decrease Epsilon: A smaller epsilon makes the model less tolerant to errors, resulting in a narrower epsilon-insensitive tube. This can lead to a smaller number of support vectors and a more constrained model.\n",
    "Gamma Parameter:\n",
    "The gamma parameter controls the influence of each training example in the SVR model. It determines the shape of the RBF kernel and affects the flexibility of the model. High gamma values lead to more complex decision boundaries, while low gamma values result in smoother decision boundaries. Consider the following:\n",
    "Increase Gamma: When the data is highly nonlinear or when you want the model to focus more on nearby points, increasing gamma can result in more intricate decision boundaries and potentially lead to overfitting.\n",
    "Decrease Gamma: When the data is relatively linear or when you want a smoother decision boundary, decreasing gamma can make the model generalize better.\n",
    "It's important to note that the optimal values for these parameters depend on the specific dataset and problem at hand. It is recommended to perform parameter tuning using techniques such as cross-validation to find the best combination of values that yields the highest performance on unseen data.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1abd881a-c0c1-4c5e-8226-75a0ba1bd018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['svm_classifier.pkl']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5.\n",
    "\n",
    "# Step 1: Import the necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "\n",
    "# Step 2: Load the dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Step 3: Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Preprocess the data (scaling using StandardScaler)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Step 5: Create an instance of the SVC classifier and train it\n",
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Use the trained classifier to predict the labels of the testing data\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Step 7: Evaluate the performance of the classifier (accuracy)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Step 8: Tune the hyperparameters using GridSearchCV\n",
    "param_grid = {'C': [0.1, 1, 10], 'gamma': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_svm = grid_search.best_estimator_\n",
    "\n",
    "# Step 9: Train the tuned classifier on the entire dataset\n",
    "best_svm.fit(X, y)\n",
    "\n",
    "# Step 10: Save the trained classifier to a file\n",
    "joblib.dump(best_svm, 'svm_classifier.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38afa8ad-3c23-4c25-96f4-b79b512035f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
