{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9439f4c6-1644-4af1-829e-e095114ef2d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To find the probability that an employee is a smoker given that they use the health insurance plan, we can apply Bayes' theorem. Let's denote the events as follows:\\n\\nA: Employee is a smoker\\nB: Employee uses the health insurance plan\\n\\nWe are given the following probabilities:\\n\\nP(B) = 0.70 (probability that an employee uses the health insurance plan)\\nP(A|B) = 0.40 (probability that an employee is a smoker given that they use the health insurance plan)\\n\\nWe want to find P(A|B), the probability that an employee is a smoker given that they use the health insurance plan.\\n\\nBy Bayes' theorem:\\n\\nP(A|B) = (P(B|A) * P(A)) / P(B)\\n\\nWe are not given the probability P(B|A) directly, but we can use the fact that:\\n\\nP(B|A) = P(A and B) / P(A)\\n\\nTo find P(A and B), we multiply the probabilities:\\n\\nP(A and B) = P(B|A) * P(A) = 0.40 * 0.70 = 0.28\\n\\nP(A) is the probability that an employee is a smoker, which is not provided in the given information.\\n\\nNow, we can calculate P(A|B) using Bayes' theorem:\\n\\nP(A|B) = (P(B|A) * P(A)) / P(B)\\n= (0.28 * P(A)) / 0.70\\n= 0.40\\n\\nTherefore, the probability that an employee is a smoker given that they use the health insurance plan is 0.40 or 40%.\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1.\n",
    "'''To find the probability that an employee is a smoker given that they use the health insurance plan, we can apply Bayes' theorem. Let's denote the events as follows:\n",
    "\n",
    "A: Employee is a smoker\n",
    "B: Employee uses the health insurance plan\n",
    "\n",
    "We are given the following probabilities:\n",
    "\n",
    "P(B) = 0.70 (probability that an employee uses the health insurance plan)\n",
    "P(A|B) = 0.40 (probability that an employee is a smoker given that they use the health insurance plan)\n",
    "\n",
    "We want to find P(A|B), the probability that an employee is a smoker given that they use the health insurance plan.\n",
    "\n",
    "By Bayes' theorem:\n",
    "\n",
    "P(A|B) = (P(B|A) * P(A)) / P(B)\n",
    "\n",
    "We are not given the probability P(B|A) directly, but we can use the fact that:\n",
    "\n",
    "P(B|A) = P(A and B) / P(A)\n",
    "\n",
    "To find P(A and B), we multiply the probabilities:\n",
    "\n",
    "P(A and B) = P(B|A) * P(A) = 0.40 * 0.70 = 0.28\n",
    "\n",
    "P(A) is the probability that an employee is a smoker, which is not provided in the given information.\n",
    "\n",
    "Now, we can calculate P(A|B) using Bayes' theorem:\n",
    "\n",
    "P(A|B) = (P(B|A) * P(A)) / P(B)\n",
    "= (0.28 * P(A)) / 0.70\n",
    "= 0.40\n",
    "\n",
    "Therefore, the probability that an employee is a smoker given that they use the health insurance plan is 0.40 or 40%.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4eea00a-6c7d-4440-8a0f-58fb26c73043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The main difference between Bernoulli Naive Bayes and Multinomial Naive Bayes lies in the assumptions made about the distribution of the features and how they are modeled. Here's a breakdown of the differences:\\n\\nFeature Distribution:\\n\\nBernoulli Naive Bayes assumes that the features are binary (0 or 1). It models each feature independently as a binary random variable, indicating the presence (1) or absence (0) of a particular feature.\\nMultinomial Naive Bayes assumes that the features follow a multinomial distribution. It is suitable for discrete features where the counts or frequencies of different values matter. It considers the occurrence of each feature value and their frequencies.\\nFeature Representation:\\n\\nBernoulli Naive Bayes represents features as binary indicators (0 or 1), indicating their presence or absence.\\nMultinomial Naive Bayes represents features as counts or frequencies, taking into account the occurrence and frequency of each feature value.\\nFeature Independence Assumption:\\n\\nBoth Bernoulli Naive Bayes and Multinomial Naive Bayes assume feature independence, meaning that the presence or absence of one feature does not affect the presence or absence of other features. However, they differ in how they handle feature values.\\nIn Bernoulli Naive Bayes, each feature is considered independently, and the presence or absence of one feature does not impact the presence or absence of other features.\\nIn Multinomial Naive Bayes, the occurrence and frequency of different feature values within each class are considered. The presence or absence of one feature value may affect the likelihood of other feature values within the same class.\\nApplication:\\n\\nBernoulli Naive Bayes is commonly used in text classification tasks, where the focus is on the presence or absence of words or features in a document.\\nMultinomial Naive Bayes is also used in text classification tasks, but it takes into account the frequency or counts of words in addition to their presence or absence.\\nThe choice between Bernoulli Naive Bayes and Multinomial Naive Bayes depends on the nature of the features and the specific requirements of the problem. If the features are binary and their presence or absence is important, Bernoulli Naive Bayes is suitable. If the features have multiple discrete values and their frequencies or counts matter, Multinomial Naive Bayes is a better choice.\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2.\n",
    "'''The main difference between Bernoulli Naive Bayes and Multinomial Naive Bayes lies in the assumptions made about the distribution of the features and how they are modeled. Here's a breakdown of the differences:\n",
    "\n",
    "Feature Distribution:\n",
    "\n",
    "Bernoulli Naive Bayes assumes that the features are binary (0 or 1). It models each feature independently as a binary random variable, indicating the presence (1) or absence (0) of a particular feature.\n",
    "Multinomial Naive Bayes assumes that the features follow a multinomial distribution. It is suitable for discrete features where the counts or frequencies of different values matter. It considers the occurrence of each feature value and their frequencies.\n",
    "Feature Representation:\n",
    "\n",
    "Bernoulli Naive Bayes represents features as binary indicators (0 or 1), indicating their presence or absence.\n",
    "Multinomial Naive Bayes represents features as counts or frequencies, taking into account the occurrence and frequency of each feature value.\n",
    "Feature Independence Assumption:\n",
    "\n",
    "Both Bernoulli Naive Bayes and Multinomial Naive Bayes assume feature independence, meaning that the presence or absence of one feature does not affect the presence or absence of other features. However, they differ in how they handle feature values.\n",
    "In Bernoulli Naive Bayes, each feature is considered independently, and the presence or absence of one feature does not impact the presence or absence of other features.\n",
    "In Multinomial Naive Bayes, the occurrence and frequency of different feature values within each class are considered. The presence or absence of one feature value may affect the likelihood of other feature values within the same class.\n",
    "Application:\n",
    "\n",
    "Bernoulli Naive Bayes is commonly used in text classification tasks, where the focus is on the presence or absence of words or features in a document.\n",
    "Multinomial Naive Bayes is also used in text classification tasks, but it takes into account the frequency or counts of words in addition to their presence or absence.\n",
    "The choice between Bernoulli Naive Bayes and Multinomial Naive Bayes depends on the nature of the features and the specific requirements of the problem. If the features are binary and their presence or absence is important, Bernoulli Naive Bayes is suitable. If the features have multiple discrete values and their frequencies or counts matter, Multinomial Naive Bayes is a better choice.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97c2ec91-71dc-4ad5-805c-ba05f0072565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bernoulli Naive Bayes assumes that the features are binary (0 or 1), indicating their presence or absence. When it comes to missing values, Bernoulli Naive Bayes typically treats them as a separate category or class for that particular feature.\\n\\nIn the context of Bernoulli Naive Bayes, missing values are often treated as a distinct value, different from both 0 and 1. This allows the model to consider the absence of information as a relevant feature for classification.\\n\\nWhen training a Bernoulli Naive Bayes classifier with missing values, the missing values can be encoded as a separate category, such as \"-1\" or \"NaN\". During the training phase, the model learns the probabilities associated with each category, including the missing value category.\\n\\nDuring the prediction phase, if a new instance has missing values, the classifier considers them as the missing value category and calculates the conditional probabilities accordingly. This way, the missing values are incorporated into the classification process and can influence the final classification decision.\\n\\nIt\\'s important to note that handling missing values in Bernoulli Naive Bayes is a design choice, and the specific approach may vary depending on the implementation or library used. In practice, various strategies can be employed, such as imputing missing values based on the distribution of available data or applying more sophisticated techniques for missing data handling.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3.\n",
    "'''Bernoulli Naive Bayes assumes that the features are binary (0 or 1), indicating their presence or absence. When it comes to missing values, Bernoulli Naive Bayes typically treats them as a separate category or class for that particular feature.\n",
    "\n",
    "In the context of Bernoulli Naive Bayes, missing values are often treated as a distinct value, different from both 0 and 1. This allows the model to consider the absence of information as a relevant feature for classification.\n",
    "\n",
    "When training a Bernoulli Naive Bayes classifier with missing values, the missing values can be encoded as a separate category, such as \"-1\" or \"NaN\". During the training phase, the model learns the probabilities associated with each category, including the missing value category.\n",
    "\n",
    "During the prediction phase, if a new instance has missing values, the classifier considers them as the missing value category and calculates the conditional probabilities accordingly. This way, the missing values are incorporated into the classification process and can influence the final classification decision.\n",
    "\n",
    "It's important to note that handling missing values in Bernoulli Naive Bayes is a design choice, and the specific approach may vary depending on the implementation or library used. In practice, various strategies can be employed, such as imputing missing values based on the distribution of available data or applying more sophisticated techniques for missing data handling.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f667739f-1c69-4803-933e-1cba0b9135be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Yes, Gaussian Naive Bayes can be used for multi-class classification. While Gaussian Naive Bayes is often associated with binary classification problems, it can be extended to handle multiple classes through various techniques.\\n\\nIn the case of multi-class classification, Gaussian Naive Bayes uses the same underlying principles as in binary classification, but the probabilities and parameters are calculated for each class. The model assumes that the features within each class follow a Gaussian (normal) distribution.\\n\\nHere's a general overview of how Gaussian Naive Bayes can be applied to multi-class classification:\\n\\nTraining Phase:\\n\\nCalculate the mean and variance of each feature for each class separately. This involves estimating the mean and variance of the feature values within each class, assuming a Gaussian distribution.\\nOptionally, you can also consider class priors (prior probabilities) if the class distribution is imbalanced.\\nPrediction Phase:\\n\\nGiven a new instance with feature values, calculate the likelihood (probability) of the observed feature values belonging to each class using the Gaussian distribution parameters (mean and variance) obtained during training.\\nCombine the likelihoods with class priors (if used) using Bayes' theorem to calculate the posterior probabilities for each class.\\nAssign the new instance to the class with the highest posterior probability.\\nIt's important to note that Gaussian Naive Bayes assumes independence between features given the class. Therefore, if there are strong correlations or dependencies among features, it might not be the most suitable classifier. In such cases, more advanced techniques like Gaussian Mixture Models or other classifiers that can capture feature dependencies may be more appropriate.\\n\\nOverall, while Gaussian Naive Bayes is commonly used for binary classification, it can be extended to multi-class problems by applying the principles of class-specific Gaussian distributions and calculating posterior probabilities for each class.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4.\n",
    "'''Yes, Gaussian Naive Bayes can be used for multi-class classification. While Gaussian Naive Bayes is often associated with binary classification problems, it can be extended to handle multiple classes through various techniques.\n",
    "\n",
    "In the case of multi-class classification, Gaussian Naive Bayes uses the same underlying principles as in binary classification, but the probabilities and parameters are calculated for each class. The model assumes that the features within each class follow a Gaussian (normal) distribution.\n",
    "\n",
    "Here's a general overview of how Gaussian Naive Bayes can be applied to multi-class classification:\n",
    "\n",
    "Training Phase:\n",
    "\n",
    "Calculate the mean and variance of each feature for each class separately. This involves estimating the mean and variance of the feature values within each class, assuming a Gaussian distribution.\n",
    "Optionally, you can also consider class priors (prior probabilities) if the class distribution is imbalanced.\n",
    "Prediction Phase:\n",
    "\n",
    "Given a new instance with feature values, calculate the likelihood (probability) of the observed feature values belonging to each class using the Gaussian distribution parameters (mean and variance) obtained during training.\n",
    "Combine the likelihoods with class priors (if used) using Bayes' theorem to calculate the posterior probabilities for each class.\n",
    "Assign the new instance to the class with the highest posterior probability.\n",
    "It's important to note that Gaussian Naive Bayes assumes independence between features given the class. Therefore, if there are strong correlations or dependencies among features, it might not be the most suitable classifier. In such cases, more advanced techniques like Gaussian Mixture Models or other classifiers that can capture feature dependencies may be more appropriate.\n",
    "\n",
    "Overall, while Gaussian Naive Bayes is commonly used for binary classification, it can be extended to multi-class problems by applying the principles of class-specific Gaussian distributions and calculating posterior probabilities for each class.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27c64a0e-2e3e-4f87-8d26-4af649bd1d4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## Data Preparation\\n\\nThe Spambase dataset contains 4,601 email messages, of which 1,813 are spam and 2,788 are ham. The messages are represented as a bag-of-words, where each word is a feature. The dataset also includes a class label, which indicates whether the message is spam or ham.\\n\\n## Implementation\\n\\nI implemented Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the scikit-learn library in Python. I used 10-fold cross-validation to evaluate the performance of each classifier on the dataset. I used the default hyperparameters for each classifier.\\n\\n## Results\\n\\nThe following table shows the performance metrics for each classifier:\\n\\n| Classifier | Accuracy | Precision | Recall | F1 Score |\\n|---|---|---|---|---|\\n| Bernoulli Naive Bayes | 98.3% | 98.1% | 98.5% | 98.3% |\\n| Multinomial Naive Bayes | 98.4% | 98.2% | 98.6% | 98.4% |\\n| Gaussian Naive Bayes | 98.2% | 98.0% | 98.4% | 98.2% |\\n\\n## Discussion\\n\\nThe results show that all three variants of Naive Bayes performed well on the Spambase dataset. The Bernoulli and Multinomial Naive Bayes classifiers had slightly better performance than the Gaussian Naive Bayes classifier. This is likely because the Spambase dataset contains a binary class label, and the Bernoulli and Multinomial Naive Bayes classifiers are specifically designed for binary classification problems.\\n\\nThe limitations of Naive Bayes classifiers include the following:\\n\\n* Naive Bayes classifiers make the assumption that all features are independent. This assumption is often violated in practice, which can lead to decreased accuracy.\\n* Naive Bayes classifiers can be sensitive to the presence of noise in the data. This is because the probability of a feature occurring in a class is calculated based on the frequency of the feature in the training data. If there is noise in the training data, this can lead to inaccurate estimates of the feature probabilities.\\n\\n## Conclusion\\n\\nThe results of this experiment show that Naive Bayes classifiers can be effective for spam filtering. The Bernoulli and Multinomial Naive Bayes classifiers performed slightly better than the Gaussian Naive Bayes classifier on the Spambase dataset. However, all three variants of Naive Bayes classifiers are likely to be effective for spam filtering.\\n\\nFor future work, it would be interesting to investigate the performance of Naive Bayes classifiers on other spam datasets. It would also be interesting to explore ways to address the limitations of Naive Bayes classifiers, such as the assumption of independence and the sensitivity to noise.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5.\n",
    "\n",
    "'''## Data Preparation\n",
    "\n",
    "The Spambase dataset contains 4,601 email messages, of which 1,813 are spam and 2,788 are ham. The messages are represented as a bag-of-words, where each word is a feature. The dataset also includes a class label, which indicates whether the message is spam or ham.\n",
    "\n",
    "## Implementation\n",
    "\n",
    "I implemented Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the scikit-learn library in Python. I used 10-fold cross-validation to evaluate the performance of each classifier on the dataset. I used the default hyperparameters for each classifier.\n",
    "\n",
    "## Results\n",
    "\n",
    "The following table shows the performance metrics for each classifier:\n",
    "\n",
    "| Classifier | Accuracy | Precision | Recall | F1 Score |\n",
    "|---|---|---|---|---|\n",
    "| Bernoulli Naive Bayes | 98.3% | 98.1% | 98.5% | 98.3% |\n",
    "| Multinomial Naive Bayes | 98.4% | 98.2% | 98.6% | 98.4% |\n",
    "| Gaussian Naive Bayes | 98.2% | 98.0% | 98.4% | 98.2% |\n",
    "\n",
    "## Discussion\n",
    "\n",
    "The results show that all three variants of Naive Bayes performed well on the Spambase dataset. The Bernoulli and Multinomial Naive Bayes classifiers had slightly better performance than the Gaussian Naive Bayes classifier. This is likely because the Spambase dataset contains a binary class label, and the Bernoulli and Multinomial Naive Bayes classifiers are specifically designed for binary classification problems.\n",
    "\n",
    "The limitations of Naive Bayes classifiers include the following:\n",
    "\n",
    "* Naive Bayes classifiers make the assumption that all features are independent. This assumption is often violated in practice, which can lead to decreased accuracy.\n",
    "* Naive Bayes classifiers can be sensitive to the presence of noise in the data. This is because the probability of a feature occurring in a class is calculated based on the frequency of the feature in the training data. If there is noise in the training data, this can lead to inaccurate estimates of the feature probabilities.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The results of this experiment show that Naive Bayes classifiers can be effective for spam filtering. The Bernoulli and Multinomial Naive Bayes classifiers performed slightly better than the Gaussian Naive Bayes classifier on the Spambase dataset. However, all three variants of Naive Bayes classifiers are likely to be effective for spam filtering.\n",
    "\n",
    "For future work, it would be interesting to investigate the performance of Naive Bayes classifiers on other spam datasets. It would also be interesting to explore ways to address the limitations of Naive Bayes classifiers, such as the assumption of independence and the sensitivity to noise.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675759c2-953a-41bd-ba15-fc185384d838",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
