{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "218a67d2-f180-46bd-823f-a116280082dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ANOVA (Analysis of Variance) is a statistical test used to analyze the differences between group means when there are more than two groups. It makes several assumptions to ensure the validity of the results. Here are the key assumptions for using ANOVA:\\n\\nIndependence: The observations within each group should be independent of each other. This means that the values in one group should not be influenced by or dependent on the values in another group.\\n\\nNormality: The data within each group should follow a normal distribution. This assumption implies that the distribution of the data in each group should be symmetric and bell-shaped.\\n\\nHomogeneity of variances: The variability (variance) of the data should be approximately equal across all groups. This assumption is called homogeneity of variances or homoscedasticity. It ensures that the groups have similar levels of dispersion.\\n\\nRandom sampling: The data should be collected through a random sampling process. This assumption allows for generalization of the results to the larger population from which the samples are drawn.\\n\\nViolations of these assumptions can impact the validity of ANOVA results. Here are examples of violations and their effects:\\n\\nViolation of independence: If the observations within groups are not independent, such as in a repeated measures design where participants are measured multiple times, the assumption is violated. This can lead to an overestimation of the statistical significance and inflated F-ratios.\\n\\nViolation of normality: If the data in any group deviates significantly from a normal distribution, the assumption is violated. This can lead to inaccurate p-values and incorrect inferences. ANOVA is robust to mild departures from normality, but severe deviations may affect the results.\\n\\nViolation of homogeneity of variances: If the variability of the data differs substantially between groups, the assumption is violated. This can distort the results, as ANOVA assumes equal variances. In such cases, alternative tests, like Welch's ANOVA or non-parametric tests, may be more appropriate.\\n\\nViolation of random sampling: If the data are not collected using a random sampling method, the generalizability of the results can be compromised. The findings may not accurately represent the larger population.\\n\\nIt is important to assess these assumptions before conducting ANOVA and take appropriate steps if violations are detected. Data transformations, non-parametric tests, or alternative statistical methods may be employed based on the nature and severity of the violations.\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1.\n",
    "'''ANOVA (Analysis of Variance) is a statistical test used to analyze the differences between group means when there are more than two groups. It makes several assumptions to ensure the validity of the results. Here are the key assumptions for using ANOVA:\n",
    "\n",
    "Independence: The observations within each group should be independent of each other. This means that the values in one group should not be influenced by or dependent on the values in another group.\n",
    "\n",
    "Normality: The data within each group should follow a normal distribution. This assumption implies that the distribution of the data in each group should be symmetric and bell-shaped.\n",
    "\n",
    "Homogeneity of variances: The variability (variance) of the data should be approximately equal across all groups. This assumption is called homogeneity of variances or homoscedasticity. It ensures that the groups have similar levels of dispersion.\n",
    "\n",
    "Random sampling: The data should be collected through a random sampling process. This assumption allows for generalization of the results to the larger population from which the samples are drawn.\n",
    "\n",
    "Violations of these assumptions can impact the validity of ANOVA results. Here are examples of violations and their effects:\n",
    "\n",
    "Violation of independence: If the observations within groups are not independent, such as in a repeated measures design where participants are measured multiple times, the assumption is violated. This can lead to an overestimation of the statistical significance and inflated F-ratios.\n",
    "\n",
    "Violation of normality: If the data in any group deviates significantly from a normal distribution, the assumption is violated. This can lead to inaccurate p-values and incorrect inferences. ANOVA is robust to mild departures from normality, but severe deviations may affect the results.\n",
    "\n",
    "Violation of homogeneity of variances: If the variability of the data differs substantially between groups, the assumption is violated. This can distort the results, as ANOVA assumes equal variances. In such cases, alternative tests, like Welch's ANOVA or non-parametric tests, may be more appropriate.\n",
    "\n",
    "Violation of random sampling: If the data are not collected using a random sampling method, the generalizability of the results can be compromised. The findings may not accurately represent the larger population.\n",
    "\n",
    "It is important to assess these assumptions before conducting ANOVA and take appropriate steps if violations are detected. Data transformations, non-parametric tests, or alternative statistical methods may be employed based on the nature and severity of the violations.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9679e97f-48aa-4d03-946c-929829a60e14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The three types of ANOVA are:\\n\\nOne-Way ANOVA: One-Way ANOVA is used when there is one categorical independent variable (factor) with more than two levels or groups, and the dependent variable is continuous. It is used to test for differences in means between the groups. For example, you might use a One-Way ANOVA to compare the average test scores of students from three different schools.\\n\\nTwo-Way ANOVA: Two-Way ANOVA is used when there are two independent variables (factors) that are categorical, and the dependent variable is continuous. It allows for the investigation of the main effects of each factor as well as the interaction effect between the two factors. For instance, you might use a Two-Way ANOVA to examine the effects of different diets (factor 1) and exercise levels (factor 2) on weight loss.\\n\\nRepeated Measures ANOVA: Repeated Measures ANOVA, also known as Within-Subjects ANOVA, is used when measurements are taken on the same subject or group of subjects under different conditions or at multiple time points. It is suitable for studying changes over time or comparing multiple treatments applied to the same subjects. For example, a Repeated Measures ANOVA could be used to analyze the effect of different doses of a drug on patients' blood pressure over several weeks.\\n\\nEach type of ANOVA is appropriate for different research designs and data structures. One-Way ANOVA is used when comparing means across multiple independent groups, Two-Way ANOVA allows for the examination of the effects of two independent variables, and Repeated Measures ANOVA is used when repeated measurements are made on the same subjects. Choosing the correct type of ANOVA depends on the specific research question and design of the study.\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2.\n",
    "'''The three types of ANOVA are:\n",
    "\n",
    "One-Way ANOVA: One-Way ANOVA is used when there is one categorical independent variable (factor) with more than two levels or groups, and the dependent variable is continuous. It is used to test for differences in means between the groups. For example, you might use a One-Way ANOVA to compare the average test scores of students from three different schools.\n",
    "\n",
    "Two-Way ANOVA: Two-Way ANOVA is used when there are two independent variables (factors) that are categorical, and the dependent variable is continuous. It allows for the investigation of the main effects of each factor as well as the interaction effect between the two factors. For instance, you might use a Two-Way ANOVA to examine the effects of different diets (factor 1) and exercise levels (factor 2) on weight loss.\n",
    "\n",
    "Repeated Measures ANOVA: Repeated Measures ANOVA, also known as Within-Subjects ANOVA, is used when measurements are taken on the same subject or group of subjects under different conditions or at multiple time points. It is suitable for studying changes over time or comparing multiple treatments applied to the same subjects. For example, a Repeated Measures ANOVA could be used to analyze the effect of different doses of a drug on patients' blood pressure over several weeks.\n",
    "\n",
    "Each type of ANOVA is appropriate for different research designs and data structures. One-Way ANOVA is used when comparing means across multiple independent groups, Two-Way ANOVA allows for the examination of the effects of two independent variables, and Repeated Measures ANOVA is used when repeated measurements are made on the same subjects. Choosing the correct type of ANOVA depends on the specific research question and design of the study.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62bc6550-6cad-4be8-98a2-87d62ab40a3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The partitioning of variance in ANOVA refers to the division of the total variance in the data into different components based on the sources of variability. This division helps in understanding the contributions of various factors and error to the overall variability observed in the data. It is a fundamental concept in ANOVA and plays a crucial role in interpreting the results correctly.\\n\\nThe partitioning of variance in ANOVA involves the following components:\\n\\nBetween-Group Variance: This component represents the variability between the group means. It measures the differences between the groups that are attributable to the independent variable(s) under investigation. In ANOVA, this variance is often referred to as the \"explained variance\" or the \"treatment effect.\"\\n\\nWithin-Group Variance: This component represents the variability within each group. It measures the differences among the individual observations within each group, unrelated to the independent variable(s). In ANOVA, this variance is often referred to as the \"error variance\" or the \"unexplained variance.\"\\n\\nTotal Variance: This component represents the overall variability in the data, regardless of the groups. It is the sum of the between-group variance and the within-group variance.\\n\\nUnderstanding the partitioning of variance is important for several reasons:\\n\\nEffectiveness of the independent variable: By partitioning the variance, ANOVA allows us to assess how much of the total variability in the dependent variable can be explained by the independent variable(s). This helps in determining the effectiveness and significance of the factors or treatments being investigated.\\n\\nHypothesis testing and significance: ANOVA utilizes the partitioned variance to calculate the F-statistic, which is used to test the null hypothesis of no group differences. The F-statistic compares the between-group variance to the within-group variance. Understanding the partitioning of variance helps in interpreting the results of the F-test correctly and assessing the statistical significance of the findings.\\n\\nInterpreting the magnitude of effects: The partitioning of variance provides insights into the relative importance and magnitude of the different sources of variability. It helps researchers understand how much of the total variability is attributable to the factors being studied and how much is due to random error.\\n\\nOverall, understanding the partitioning of variance in ANOVA enables researchers to effectively analyze and interpret the sources of variability in their data, evaluate the significance of the factors being studied, and draw valid conclusions about group differences or treatment effects.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3.\n",
    "'''The partitioning of variance in ANOVA refers to the division of the total variance in the data into different components based on the sources of variability. This division helps in understanding the contributions of various factors and error to the overall variability observed in the data. It is a fundamental concept in ANOVA and plays a crucial role in interpreting the results correctly.\n",
    "\n",
    "The partitioning of variance in ANOVA involves the following components:\n",
    "\n",
    "Between-Group Variance: This component represents the variability between the group means. It measures the differences between the groups that are attributable to the independent variable(s) under investigation. In ANOVA, this variance is often referred to as the \"explained variance\" or the \"treatment effect.\"\n",
    "\n",
    "Within-Group Variance: This component represents the variability within each group. It measures the differences among the individual observations within each group, unrelated to the independent variable(s). In ANOVA, this variance is often referred to as the \"error variance\" or the \"unexplained variance.\"\n",
    "\n",
    "Total Variance: This component represents the overall variability in the data, regardless of the groups. It is the sum of the between-group variance and the within-group variance.\n",
    "\n",
    "Understanding the partitioning of variance is important for several reasons:\n",
    "\n",
    "Effectiveness of the independent variable: By partitioning the variance, ANOVA allows us to assess how much of the total variability in the dependent variable can be explained by the independent variable(s). This helps in determining the effectiveness and significance of the factors or treatments being investigated.\n",
    "\n",
    "Hypothesis testing and significance: ANOVA utilizes the partitioned variance to calculate the F-statistic, which is used to test the null hypothesis of no group differences. The F-statistic compares the between-group variance to the within-group variance. Understanding the partitioning of variance helps in interpreting the results of the F-test correctly and assessing the statistical significance of the findings.\n",
    "\n",
    "Interpreting the magnitude of effects: The partitioning of variance provides insights into the relative importance and magnitude of the different sources of variability. It helps researchers understand how much of the total variability is attributable to the factors being studied and how much is due to random error.\n",
    "\n",
    "Overall, understanding the partitioning of variance in ANOVA enables researchers to effectively analyze and interpret the sources of variability in their data, evaluate the significance of the factors being studied, and draw valid conclusions about group differences or treatment effects.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a5db3b6-72c7-43e7-99f9-1fc5d1e39930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SST: 280.0\n",
      "SSE: 250.0\n",
      "SSR: 30.0\n"
     ]
    }
   ],
   "source": [
    "#4.\n",
    "\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Define the data for each group\n",
    "group1 = [1, 2, 3, 4, 5]\n",
    "group2 = [6, 7, 8, 9, 10]\n",
    "group3 = [11, 12, 13, 14, 15]\n",
    "\n",
    "# Combine the data into a single list\n",
    "data = group1 + group2 + group3\n",
    "\n",
    "# Calculate the mean of the data\n",
    "mean = sum(data) / len(data)\n",
    "\n",
    "# Calculate the total sum of squares (SST)\n",
    "sst = sum((x - mean) ** 2 for x in data)\n",
    "\n",
    "# Calculate the explained sum of squares (SSE)\n",
    "group_means = [sum(group) / len(group) for group in [group1, group2, group3]]\n",
    "sse = sum(len(group) * (group_mean - mean) ** 2 for group, group_mean in zip([group1, group2, group3], group_means))\n",
    "\n",
    "# Calculate the residual sum of squares (SSR)\n",
    "ssr = sst - sse\n",
    "\n",
    "print(\"SST:\", sst)\n",
    "print(\"SSE:\", sse)\n",
    "print(\"SSR:\", ssr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8dca328-5abf-4539-94a4-5d25f6d5a5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Effect A: 10.000000000000027\n",
      "Main Effect B: 3.1554436208840472e-30\n",
      "Interaction Effect: 7.888609052210118e-31\n"
     ]
    }
   ],
   "source": [
    "#5.\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Create a dataframe with the data for the two independent variables (factors) and the dependent variable\n",
    "data = pd.DataFrame({'A': [1, 2, 3, 4, 5],\n",
    "                     'B': [6, 7, 8, 9, 10],\n",
    "                     'Y': [11, 12, 13, 14, 15]})\n",
    "\n",
    "# Fit the two-way ANOVA model\n",
    "model = ols('Y ~ A + B + A:B', data=data).fit()\n",
    "anova_table = sm.stats.anova_lm(model)\n",
    "\n",
    "# Extract the main effects and interaction effect from the ANOVA table\n",
    "main_effect_A = anova_table['sum_sq']['A']\n",
    "main_effect_B = anova_table['sum_sq']['B']\n",
    "interaction_effect = anova_table['sum_sq']['A:B']\n",
    "\n",
    "print(\"Main Effect A:\", main_effect_A)\n",
    "print(\"Main Effect B:\", main_effect_B)\n",
    "print(\"Interaction Effect:\", interaction_effect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c69943ab-0d5f-4bf0-8edd-392ca9f8603c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In the given scenario, where a one-way ANOVA was conducted and the obtained F-statistic is 5.23 with a p-value of 0.02, we can draw conclusions about the differences between the groups. Here's how you can interpret these results:\\n\\nF-Statistic: The F-statistic is a measure of the ratio of the variability between the groups (explained variance) to the variability within the groups (unexplained variance). In this case, the F-statistic is 5.23.\\n\\nP-value: The p-value is the probability of observing a test statistic as extreme as the one obtained, assuming the null hypothesis is true. In this case, the p-value is 0.02.\\n\\nInterpretation:\\n\\nSince the p-value (0.02) is less than the commonly used significance level of 0.05, we can reject the null hypothesis. This indicates that there are significant differences between at least two of the groups in the study.\\n\\nAdditionally, the F-statistic (5.23) provides further evidence to support the rejection of the null hypothesis. A higher F-statistic suggests a larger difference between the group means compared to the variability within the groups.\\n\\nTherefore, based on these results, we can conclude that there are statistically significant differences between the groups being compared in the study. However, it is important to note that the one-way ANOVA does not identify which specific groups differ from each other. Post-hoc tests, such as Tukey's test or pairwise comparisons, can be conducted to determine the specific group differences if needed.\\n\\nRemember to consider the context of the study and the specific research question when interpreting the results.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#6.\n",
    "'''In the given scenario, where a one-way ANOVA was conducted and the obtained F-statistic is 5.23 with a p-value of 0.02, we can draw conclusions about the differences between the groups. Here's how you can interpret these results:\n",
    "\n",
    "F-Statistic: The F-statistic is a measure of the ratio of the variability between the groups (explained variance) to the variability within the groups (unexplained variance). In this case, the F-statistic is 5.23.\n",
    "\n",
    "P-value: The p-value is the probability of observing a test statistic as extreme as the one obtained, assuming the null hypothesis is true. In this case, the p-value is 0.02.\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "Since the p-value (0.02) is less than the commonly used significance level of 0.05, we can reject the null hypothesis. This indicates that there are significant differences between at least two of the groups in the study.\n",
    "\n",
    "Additionally, the F-statistic (5.23) provides further evidence to support the rejection of the null hypothesis. A higher F-statistic suggests a larger difference between the group means compared to the variability within the groups.\n",
    "\n",
    "Therefore, based on these results, we can conclude that there are statistically significant differences between the groups being compared in the study. However, it is important to note that the one-way ANOVA does not identify which specific groups differ from each other. Post-hoc tests, such as Tukey's test or pairwise comparisons, can be conducted to determine the specific group differences if needed.\n",
    "\n",
    "Remember to consider the context of the study and the specific research question when interpreting the results.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5f98e34-cffd-4d95-bbce-08b2529ef572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Handling missing data in a repeated measures ANOVA requires careful consideration, as missing data can potentially introduce bias and impact the validity of the results. There are various methods to handle missing data, each with its own implications. Here are a few common approaches:\\n\\nComplete Case Analysis (Listwise Deletion): This approach involves excluding any participants or cases with missing data from the analysis. It analyzes only the complete cases, thereby reducing the sample size. The main advantage is that it provides unbiased estimates if the missing data are missing completely at random (MCAR). However, it may lead to loss of statistical power and potential bias if the missingness is related to the variables under study.\\n\\nPairwise Deletion: This method includes all available data by analyzing each participant's available data points. It uses all available information while estimating means and variances. However, it can lead to biased estimates if the missingness is related to the variables under study or if the data are not missing completely at random (MCAR).\\n\\nImputation: Imputation involves replacing missing values with estimated values. Common imputation methods include mean imputation, last observation carried forward (LOCF), regression imputation, or multiple imputation. Imputation helps retain sample size and can reduce bias if the imputation model accurately captures the underlying relationships in the data. However, imputation assumes the data are missing at random (MAR) and may introduce additional uncertainty if the imputation model is misspecified.\\n\\nThe choice of method depends on the nature and pattern of missing data, as well as assumptions made about the missingness mechanism. It is crucial to carefully assess the missing data mechanism and consider the potential consequences of different methods:\\n\\nBias: Using inappropriate methods can introduce bias in the estimates. For example, complete case analysis or pairwise deletion may introduce bias if the missingness is related to the variables being studied.\\n\\nPrecision: Different methods can affect the precision or variability of the estimates. For instance, imputation methods may reduce variability, but they may also introduce additional uncertainty.\\n\\nStatistical Power: The choice of method can impact the statistical power of the analysis. Complete case analysis or pairwise deletion may result in reduced power due to reduced sample size, while imputation methods may preserve power by retaining the sample size.\\n\\nAssumptions: Each method assumes a particular missing data mechanism (e.g., MCAR, MAR). Violating these assumptions can affect the validity of the results obtained using that method.\\n\\nIn practice, it is recommended to explore the missing data pattern, consider the underlying missingness mechanism, and apply appropriate missing data handling techniques. Sensitivity analyses and multiple imputation approaches can be employed to assess the robustness of the results across different missing data assumptions and methods.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#7.\n",
    "'''Handling missing data in a repeated measures ANOVA requires careful consideration, as missing data can potentially introduce bias and impact the validity of the results. There are various methods to handle missing data, each with its own implications. Here are a few common approaches:\n",
    "\n",
    "Complete Case Analysis (Listwise Deletion): This approach involves excluding any participants or cases with missing data from the analysis. It analyzes only the complete cases, thereby reducing the sample size. The main advantage is that it provides unbiased estimates if the missing data are missing completely at random (MCAR). However, it may lead to loss of statistical power and potential bias if the missingness is related to the variables under study.\n",
    "\n",
    "Pairwise Deletion: This method includes all available data by analyzing each participant's available data points. It uses all available information while estimating means and variances. However, it can lead to biased estimates if the missingness is related to the variables under study or if the data are not missing completely at random (MCAR).\n",
    "\n",
    "Imputation: Imputation involves replacing missing values with estimated values. Common imputation methods include mean imputation, last observation carried forward (LOCF), regression imputation, or multiple imputation. Imputation helps retain sample size and can reduce bias if the imputation model accurately captures the underlying relationships in the data. However, imputation assumes the data are missing at random (MAR) and may introduce additional uncertainty if the imputation model is misspecified.\n",
    "\n",
    "The choice of method depends on the nature and pattern of missing data, as well as assumptions made about the missingness mechanism. It is crucial to carefully assess the missing data mechanism and consider the potential consequences of different methods:\n",
    "\n",
    "Bias: Using inappropriate methods can introduce bias in the estimates. For example, complete case analysis or pairwise deletion may introduce bias if the missingness is related to the variables being studied.\n",
    "\n",
    "Precision: Different methods can affect the precision or variability of the estimates. For instance, imputation methods may reduce variability, but they may also introduce additional uncertainty.\n",
    "\n",
    "Statistical Power: The choice of method can impact the statistical power of the analysis. Complete case analysis or pairwise deletion may result in reduced power due to reduced sample size, while imputation methods may preserve power by retaining the sample size.\n",
    "\n",
    "Assumptions: Each method assumes a particular missing data mechanism (e.g., MCAR, MAR). Violating these assumptions can affect the validity of the results obtained using that method.\n",
    "\n",
    "In practice, it is recommended to explore the missing data pattern, consider the underlying missingness mechanism, and apply appropriate missing data handling techniques. Sensitivity analyses and multiple imputation approaches can be employed to assess the robustness of the results across different missing data assumptions and methods.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ec0bd32-41ee-43bb-996d-7a64213b17bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"After conducting an ANOVA and finding a significant overall effect, post-hoc tests are used to determine which specific group differences are significant. Several common post-hoc tests are available, and the choice depends on factors such as the study design, assumptions, and objectives. Here are some commonly used post-hoc tests:\\n\\nTukey's Honestly Significant Difference (HSD) test: Tukey's test compares all possible pairs of group means to identify significant differences. It controls the experiment-wise error rate, making it suitable for situations where multiple pairwise comparisons are conducted. Tukey's test is conservative and often recommended when the group sizes are equal.\\n\\nBonferroni correction: This is a conservative method that adjusts the significance level for multiple comparisons. It divides the desired significance level (e.g., 0.05) by the number of comparisons to obtain a stricter critical value. It is suitable when controlling for the family-wise error rate is crucial.\\n\\nScheffé's test: Scheffé's test is more liberal than Tukey's test and is appropriate when the group sizes are unequal. It is less restrictive and has a wider confidence interval, making it useful for exploratory analyses or situations with small sample sizes.\\n\\nDunnett's test: Dunnett's test compares each group mean with a control group mean. It is used when there is a predefined control group, and the focus is on comparing other groups to the control.\\n\\nExample situation:\\nSuppose a study compares the effects of three different exercise programs (Group A, Group B, and Group C) on cardiovascular fitness. After conducting an ANOVA, if the overall F-test suggests significant differences between the groups, a post-hoc test would be necessary to identify which specific group differences are significant.\\n\\nFor instance, Tukey's HSD test could be used to compare all possible pairs of group means. The test would determine if there are significant differences in cardiovascular fitness between Group A and Group B, Group A and Group C, and Group B and Group C. This would provide a more detailed understanding of the specific exercise programs that lead to significant differences in cardiovascular fitness.\\n\\nThe choice of the post-hoc test depends on factors such as the study design, the number of groups, sample sizes, and the specific hypotheses of interest. It is important to select an appropriate post-hoc test that suits the research question and considers the underlying assumptions and requirements of the analysis.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#8.\n",
    "'''After conducting an ANOVA and finding a significant overall effect, post-hoc tests are used to determine which specific group differences are significant. Several common post-hoc tests are available, and the choice depends on factors such as the study design, assumptions, and objectives. Here are some commonly used post-hoc tests:\n",
    "\n",
    "Tukey's Honestly Significant Difference (HSD) test: Tukey's test compares all possible pairs of group means to identify significant differences. It controls the experiment-wise error rate, making it suitable for situations where multiple pairwise comparisons are conducted. Tukey's test is conservative and often recommended when the group sizes are equal.\n",
    "\n",
    "Bonferroni correction: This is a conservative method that adjusts the significance level for multiple comparisons. It divides the desired significance level (e.g., 0.05) by the number of comparisons to obtain a stricter critical value. It is suitable when controlling for the family-wise error rate is crucial.\n",
    "\n",
    "Scheffé's test: Scheffé's test is more liberal than Tukey's test and is appropriate when the group sizes are unequal. It is less restrictive and has a wider confidence interval, making it useful for exploratory analyses or situations with small sample sizes.\n",
    "\n",
    "Dunnett's test: Dunnett's test compares each group mean with a control group mean. It is used when there is a predefined control group, and the focus is on comparing other groups to the control.\n",
    "\n",
    "Example situation:\n",
    "Suppose a study compares the effects of three different exercise programs (Group A, Group B, and Group C) on cardiovascular fitness. After conducting an ANOVA, if the overall F-test suggests significant differences between the groups, a post-hoc test would be necessary to identify which specific group differences are significant.\n",
    "\n",
    "For instance, Tukey's HSD test could be used to compare all possible pairs of group means. The test would determine if there are significant differences in cardiovascular fitness between Group A and Group B, Group A and Group C, and Group B and Group C. This would provide a more detailed understanding of the specific exercise programs that lead to significant differences in cardiovascular fitness.\n",
    "\n",
    "The choice of the post-hoc test depends on factors such as the study design, the number of groups, sample sizes, and the specific hypotheses of interest. It is important to select an appropriate post-hoc test that suits the research question and considers the underlying assumptions and requirements of the analysis.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10ef2d87-ef82-4f4a-ad08-e172e30c53f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-Statistic: 511.7406209573102\n",
      "p-value: 2.2782763298692447e-65\n"
     ]
    }
   ],
   "source": [
    "#9.\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Weight loss data for each diet group\n",
    "diet_A = [2.5, 3.1, 2.8, 3.5, 2.9, 3.2, 3.4, 2.6, 2.9, 3.3, 3.7, 3.1, 2.8, 3.0, 2.7, 3.2, 2.9, 3.5, 3.1, 2.8,\n",
    "          2.9, 3.3, 3.0, 2.7, 3.2, 2.9, 3.1, 2.8, 3.5, 3.2, 2.9, 3.1, 2.8, 3.0, 2.7, 3.2, 2.9, 3.1, 2.8, 3.5,\n",
    "          2.9, 3.2, 3.4, 2.6, 2.9, 3.3, 3.7, 3.1]\n",
    "\n",
    "diet_B = [1.8, 2.1, 2.0, 1.7, 2.2, 2.3, 2.1, 1.9, 2.4, 2.0, 2.1, 2.2, 2.3, 1.9, 2.1, 2.2, 1.8, 2.1, 2.0, 1.7,\n",
    "          2.2, 2.3, 2.1, 1.9, 2.4, 2.0, 2.1, 2.2, 2.3, 1.9, 2.1, 2.2, 1.8, 2.1, 2.0, 1.7, 2.2, 2.3, 2.1, 1.9,\n",
    "          2.4, 2.0, 2.1, 2.2, 2.3, 1.9, 2.1, 2.2]\n",
    "\n",
    "diet_C = [1.5, 1.7, 1.6, 1.4, 1.8, 1.9, 1.7, 1.5, 1.6, 1.8, 1.9, 1.7, 1.5, 1.6, 1.4, 1.8, 1.9, 1.7, 1.5, 1.6,\n",
    "          1.8, 1.9, 1.7, 1.5, 1.6, 1.4, 1.8, 1.9, 1.7, 1.5, 1.6, 1.4, 1.8, 1.9, 1.7, 1.5, 1.6, 1.8, 1.9, 1.7,\n",
    "          1.5, 1.6, 1.4, 1.8, 1.9, 1.7, 1.5, 1.6]\n",
    "\n",
    "# Perform one-way ANOVA\n",
    "f_statistic, p_value = stats.f_oneway(diet_A, diet_B, diet_C)\n",
    "\n",
    "print(\"F-Statistic:\", f_statistic)\n",
    "print(\"p-value:\", p_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecde0986-2753-420b-905d-c01ab13fe95d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     sum_sq    df         F        PR(>F)\n",
      "Software             1904.4   2.0  51.65642  9.931558e-08\n",
      "Experience              NaN   1.0       NaN           NaN\n",
      "Software:Experience     0.4   2.0   0.01085  9.178100e-01\n",
      "Residual              497.7  27.0       NaN           NaN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/statsmodels/base/model.py:1871: ValueWarning: covariance of constraints does not have full rank. The number of constraints is 2, but rank is 1\n",
      "  warnings.warn('covariance of constraints does not have full '\n",
      "/opt/conda/lib/python3.10/site-packages/statsmodels/base/model.py:1871: ValueWarning: covariance of constraints does not have full rank. The number of constraints is 1, but rank is 0\n",
      "  warnings.warn('covariance of constraints does not have full '\n",
      "/opt/conda/lib/python3.10/site-packages/statsmodels/base/model.py:1900: RuntimeWarning: invalid value encountered in divide\n",
      "  F /= J\n",
      "/opt/conda/lib/python3.10/site-packages/statsmodels/base/model.py:1871: ValueWarning: covariance of constraints does not have full rank. The number of constraints is 2, but rank is 1\n",
      "  warnings.warn('covariance of constraints does not have full '\n"
     ]
    }
   ],
   "source": [
    "#10.\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Create a dataframe with the data\n",
    "data = {\n",
    "    'Software': ['A', 'B', 'C'] * 10,\n",
    "    'Experience': ['Novice', 'Novice', 'Experienced'] * 10,\n",
    "    'Time': [12, 14, 15, 18, 17, 20, 22, 23, 21, 20,\n",
    "             14, 15, 16, 17, 19, 20, 23, 25, 21, 22,\n",
    "             16, 18, 19, 21, 23, 24, 26, 28, 27, 25]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Perform two-way ANOVA\n",
    "model = ols('Time ~ Software + Experience + Software:Experience', data=df).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "print(anova_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f117a6c-28b2-4915-9093-98a3d760b3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-Statistic: -12.524802499714966\n",
      "p-value: 3.3078072965913897e-22\n"
     ]
    }
   ],
   "source": [
    "#11.\n",
    "\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "\n",
    "# Test scores for the control group and experimental group\n",
    "control_group = np.array([80, 75, 85, 72, 78, 82, 77, 75, 80, 79, 76, 81, 78, 77, 80, 83, 79, 81, 78, 76, \n",
    "                         77, 74, 75, 79, 80, 82, 77, 76, 78, 79, 76, 80, 83, 78, 75, 82, 79, 80, 77, 76, \n",
    "                         78, 81, 79, 77, 80, 78, 76, 75, 79, 81, 77, 80])\n",
    "\n",
    "experimental_group = np.array([85, 82, 89, 86, 83, 88, 84, 81, 87, 86, 85, 82, 89, 87, 83, 85, 82, 84, 81, 86, \n",
    "                              88, 89, 83, 85, 82, 84, 81, 87, 86, 85, 82, 88, 84, 83, 85, 82, 84, 81, 86, 87, \n",
    "                              85, 82, 89, 84, 81, 83, 88, 86, 85, 82])\n",
    "\n",
    "# Perform two-sample t-test\n",
    "t_statistic, p_value = stats.ttest_ind(control_group, experimental_group)\n",
    "\n",
    "print(\"t-Statistic:\", t_statistic)\n",
    "print(\"p-value:\", p_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41c8c984-1f3f-4aad-9e61-bedd5e3ff5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               sum_sq    df           F        PR(>F)\n",
      "Store     4862.688889   2.0  567.147109  8.400248e-39\n",
      "C(Day)     108.988889  29.0    0.876665  6.435061e-01\n",
      "Residual   248.644444  58.0         NaN           NaN\n"
     ]
    }
   ],
   "source": [
    "#12.\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Create a dataframe with the data\n",
    "data = {\n",
    "    'Day': range(1, 31),\n",
    "    'Store_A': [100, 95, 102, 105, 98, 102, 99, 101, 100, 97, 103, 98, 101, 100, 98, 103, 105, 102, 98, 100,\n",
    "                96, 103, 102, 99, 100, 101, 98, 105, 102, 101],\n",
    "    'Store_B': [90, 92, 88, 91, 93, 90, 92, 94, 88, 90, 92, 89, 91, 90, 93, 92, 90, 92, 91, 89,\n",
    "                93, 94, 91, 92, 90, 89, 91, 92, 94, 90],\n",
    "    'Store_C': [80, 85, 82, 81, 85, 82, 83, 80, 84, 82, 81, 85, 80, 83, 82, 84, 80, 85, 82, 83,\n",
    "                81, 84, 85, 82, 83, 81, 80, 84, 82, 83]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Transform the dataframe to long format\n",
    "df_long = pd.melt(df, id_vars='Day', value_vars=['Store_A', 'Store_B', 'Store_C'],\n",
    "                  var_name='Store', value_name='Sales')\n",
    "\n",
    "# Perform repeated measures ANOVA\n",
    "model = ols('Sales ~ Store + C(Day)', data=df_long).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "print(anova_table)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f917c97-94d6-4a6e-aded-be8ff0848924",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
