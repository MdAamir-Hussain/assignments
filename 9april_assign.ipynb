{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0767d0aa-4adb-403f-b5f9-30a7e93153e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Bayes' theorem, named after the English mathematician and philosopher Thomas Bayes, is a fundamental concept in probability theory and statistics. It provides a way to calculate the probability of an event based on prior knowledge or beliefs, as well as new evidence.\\n\\nMathematically, Bayes' theorem can be stated as follows:\\n\\nP(A|B) = (P(B|A) * P(A)) / P(B)\\n\\nWhere:\\n\\nP(A|B) represents the conditional probability of event A occurring given that event B has occurred.\\nP(B|A) represents the conditional probability of event B occurring given that event A has occurred.\\nP(A) is the probability of event A occurring.\\nP(B) is the probability of event B occurring.\\nIn other words, Bayes' theorem allows us to update our beliefs about the probability of an event A, given new evidence B. It relates the conditional probabilities of A given B and B given A to the individual probabilities of A and B.\\n\\nBayes' theorem has a wide range of applications, including but not limited to:\\n\\nMedical diagnosis: Assessing the probability of a disease given certain symptoms and test results.\\nSpam filtering: Determining the probability of an email being spam based on its content.\\nMachine learning: Bayesian inference is used in many algorithms for probabilistic modeling and prediction.\\nNatural language processing: Bayesian methods can be applied to language modeling and text classification tasks.\\nBayes' theorem provides a formal framework for updating beliefs and reasoning under uncertainty, making it a powerful tool in various fields of study.\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1.\n",
    "'''Bayes' theorem, named after the English mathematician and philosopher Thomas Bayes, is a fundamental concept in probability theory and statistics. It provides a way to calculate the probability of an event based on prior knowledge or beliefs, as well as new evidence.\n",
    "\n",
    "Mathematically, Bayes' theorem can be stated as follows:\n",
    "\n",
    "P(A|B) = (P(B|A) * P(A)) / P(B)\n",
    "\n",
    "Where:\n",
    "\n",
    "P(A|B) represents the conditional probability of event A occurring given that event B has occurred.\n",
    "P(B|A) represents the conditional probability of event B occurring given that event A has occurred.\n",
    "P(A) is the probability of event A occurring.\n",
    "P(B) is the probability of event B occurring.\n",
    "In other words, Bayes' theorem allows us to update our beliefs about the probability of an event A, given new evidence B. It relates the conditional probabilities of A given B and B given A to the individual probabilities of A and B.\n",
    "\n",
    "Bayes' theorem has a wide range of applications, including but not limited to:\n",
    "\n",
    "Medical diagnosis: Assessing the probability of a disease given certain symptoms and test results.\n",
    "Spam filtering: Determining the probability of an email being spam based on its content.\n",
    "Machine learning: Bayesian inference is used in many algorithms for probabilistic modeling and prediction.\n",
    "Natural language processing: Bayesian methods can be applied to language modeling and text classification tasks.\n",
    "Bayes' theorem provides a formal framework for updating beliefs and reasoning under uncertainty, making it a powerful tool in various fields of study.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d70bd7b4-9fac-4def-8721-2b76159804ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The formula for Bayes' theorem is as follows:\\n\\nP(A|B) = (P(B|A) * P(A)) / P(B)\\n\\nWhere:\\n\\nP(A|B) represents the conditional probability of event A occurring given that event B has occurred.\\nP(B|A) represents the conditional probability of event B occurring given that event A has occurred.\\nP(A) is the probability of event A occurring.\\nP(B) is the probability of event B occurring.\\nBayes' theorem allows us to update our beliefs about the probability of an event A, given new evidence B, by relating the conditional probabilities of A given B and B given A to the individual probabilities of A and B. It provides a formal framework for updating beliefs and reasoning under uncertainty.\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2.\n",
    "'''The formula for Bayes' theorem is as follows:\n",
    "\n",
    "P(A|B) = (P(B|A) * P(A)) / P(B)\n",
    "\n",
    "Where:\n",
    "\n",
    "P(A|B) represents the conditional probability of event A occurring given that event B has occurred.\n",
    "P(B|A) represents the conditional probability of event B occurring given that event A has occurred.\n",
    "P(A) is the probability of event A occurring.\n",
    "P(B) is the probability of event B occurring.\n",
    "Bayes' theorem allows us to update our beliefs about the probability of an event A, given new evidence B, by relating the conditional probabilities of A given B and B given A to the individual probabilities of A and B. It provides a formal framework for updating beliefs and reasoning under uncertainty.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9e078d8-af31-43fa-b709-283b6580821b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Bayes' theorem is widely used in practice across various fields. Here are a few examples of how it is applied:\\n\\nMedical Diagnosis: Bayes' theorem is used to assess the probability of a particular disease given certain symptoms or test results. Medical professionals can update the probability of a disease based on the patient's symptoms and the likelihood of those symptoms occurring in patients with and without the disease.\\n\\nSpam Filtering: Bayes' theorem is employed in spam filters to determine the probability of an email being spam based on its content. The filter analyzes various features of the email (e.g., keywords, patterns, sender information) and calculates the likelihood of it being spam or legitimate mail.\\n\\nPredictive Modeling: In machine learning and statistical modeling, Bayesian inference is used to update prior beliefs about model parameters based on observed data. Bayesian methods provide a way to incorporate prior knowledge or beliefs into the modeling process and make more accurate predictions.\\n\\nA/B Testing: Bayes' theorem is used to analyze the results of A/B tests, where two different versions of a webpage, application, or campaign are compared. By calculating the posterior probability of one version being better than the other, Bayesian inference helps in decision-making regarding which version to choose.\\n\\nNatural Language Processing: Bayesian methods are employed in tasks such as language modeling, text classification, and sentiment analysis. By incorporating prior knowledge about word frequencies, document categories, or sentiment distributions, Bayesian models can improve the accuracy of natural language processing tasks.\\n\\nThese are just a few examples, but Bayes' theorem has applications in many other areas, including finance, economics, genetics, robotics, and more. Its versatility and ability to update probabilities based on new evidence make it a valuable tool in decision-making and reasoning under uncertainty.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3.\n",
    "'''Bayes' theorem is widely used in practice across various fields. Here are a few examples of how it is applied:\n",
    "\n",
    "Medical Diagnosis: Bayes' theorem is used to assess the probability of a particular disease given certain symptoms or test results. Medical professionals can update the probability of a disease based on the patient's symptoms and the likelihood of those symptoms occurring in patients with and without the disease.\n",
    "\n",
    "Spam Filtering: Bayes' theorem is employed in spam filters to determine the probability of an email being spam based on its content. The filter analyzes various features of the email (e.g., keywords, patterns, sender information) and calculates the likelihood of it being spam or legitimate mail.\n",
    "\n",
    "Predictive Modeling: In machine learning and statistical modeling, Bayesian inference is used to update prior beliefs about model parameters based on observed data. Bayesian methods provide a way to incorporate prior knowledge or beliefs into the modeling process and make more accurate predictions.\n",
    "\n",
    "A/B Testing: Bayes' theorem is used to analyze the results of A/B tests, where two different versions of a webpage, application, or campaign are compared. By calculating the posterior probability of one version being better than the other, Bayesian inference helps in decision-making regarding which version to choose.\n",
    "\n",
    "Natural Language Processing: Bayesian methods are employed in tasks such as language modeling, text classification, and sentiment analysis. By incorporating prior knowledge about word frequencies, document categories, or sentiment distributions, Bayesian models can improve the accuracy of natural language processing tasks.\n",
    "\n",
    "These are just a few examples, but Bayes' theorem has applications in many other areas, including finance, economics, genetics, robotics, and more. Its versatility and ability to update probabilities based on new evidence make it a valuable tool in decision-making and reasoning under uncertainty.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffa6f98f-58c0-4fd5-9eb6-1a79f46560a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Bayes' theorem and conditional probability are closely related concepts. In fact, Bayes' theorem is derived from the principles of conditional probability.\\n\\nConditional probability refers to the probability of an event A occurring given that another event B has already occurred. It is denoted as P(A|B) and can be calculated as the ratio of the probability of the intersection of events A and B (P(A ∩ B)) to the probability of event B (P(B)):\\n\\nP(A|B) = P(A ∩ B) / P(B)\\n\\nBayes' theorem builds upon this notion of conditional probability and provides a way to update our beliefs about the probability of an event based on new evidence. It relates the conditional probability of event A given B (P(A|B)) to the conditional probability of event B given A (P(B|A)) and the individual probabilities of A and B:\\n\\nP(A|B) = (P(B|A) * P(A)) / P(B)\\n\\nIn this formula, P(B|A) represents the conditional probability of event B occurring given that event A has occurred, P(A) is the probability of event A occurring, and P(B) is the probability of event B occurring.\\n\\nSo, while conditional probability provides the foundation for understanding the likelihood of an event given another event, Bayes' theorem extends this by incorporating prior probabilities and enabling the updating of beliefs based on new evidence.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4.\n",
    "'''Bayes' theorem and conditional probability are closely related concepts. In fact, Bayes' theorem is derived from the principles of conditional probability.\n",
    "\n",
    "Conditional probability refers to the probability of an event A occurring given that another event B has already occurred. It is denoted as P(A|B) and can be calculated as the ratio of the probability of the intersection of events A and B (P(A ∩ B)) to the probability of event B (P(B)):\n",
    "\n",
    "P(A|B) = P(A ∩ B) / P(B)\n",
    "\n",
    "Bayes' theorem builds upon this notion of conditional probability and provides a way to update our beliefs about the probability of an event based on new evidence. It relates the conditional probability of event A given B (P(A|B)) to the conditional probability of event B given A (P(B|A)) and the individual probabilities of A and B:\n",
    "\n",
    "P(A|B) = (P(B|A) * P(A)) / P(B)\n",
    "\n",
    "In this formula, P(B|A) represents the conditional probability of event B occurring given that event A has occurred, P(A) is the probability of event A occurring, and P(B) is the probability of event B occurring.\n",
    "\n",
    "So, while conditional probability provides the foundation for understanding the likelihood of an event given another event, Bayes' theorem extends this by incorporating prior probabilities and enabling the updating of beliefs based on new evidence.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fdeab92-416a-4355-bfb0-a33a6a37667e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'When choosing which type of Naive Bayes classifier to use for a specific problem, you typically consider the characteristics of your data and the assumptions made by each classifier variant. The three commonly used Naive Bayes classifiers are:\\n\\nGaussian Naive Bayes:\\n\\nAssumption: Assumes that the continuous features in the data follow a Gaussian (normal) distribution.\\nUse case: Suitable for problems where the continuous features can be assumed to have a Gaussian distribution, such as when dealing with real-valued or continuous data.\\nMultinomial Naive Bayes:\\n\\nAssumption: Assumes that the features in the data are generated from a multinomial distribution, which is appropriate for discrete features.\\nUse case: Typically used for text classification problems, where the features represent word counts or term frequencies in documents.\\nBernoulli Naive Bayes:\\n\\nAssumption: Assumes that the features are binary (0 or 1) and follow a Bernoulli distribution.\\nUse case: Appropriate when dealing with binary features, such as presence or absence of certain attributes.\\nTo decide which Naive Bayes classifier to use, consider the nature of your features and their distribution. Here are some general guidelines:\\n\\nGaussian Naive Bayes is suitable for continuous features that can be assumed to follow a Gaussian distribution, such as when dealing with numerical data.\\nMultinomial Naive Bayes is appropriate when working with discrete features or text data where word frequencies or counts are relevant.\\nBernoulli Naive Bayes is useful when dealing with binary features, such as presence or absence of certain attributes.\\nHowever, these are general guidelines, and the choice also depends on the specific characteristics of your dataset and the problem at hand. It is recommended to experiment with different variants and evaluate their performance using suitable evaluation metrics, cross-validation, or other techniques to determine the most appropriate Naive Bayes classifier for your specific problem.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5.\n",
    "'''When choosing which type of Naive Bayes classifier to use for a specific problem, you typically consider the characteristics of your data and the assumptions made by each classifier variant. The three commonly used Naive Bayes classifiers are:\n",
    "\n",
    "Gaussian Naive Bayes:\n",
    "\n",
    "Assumption: Assumes that the continuous features in the data follow a Gaussian (normal) distribution.\n",
    "Use case: Suitable for problems where the continuous features can be assumed to have a Gaussian distribution, such as when dealing with real-valued or continuous data.\n",
    "Multinomial Naive Bayes:\n",
    "\n",
    "Assumption: Assumes that the features in the data are generated from a multinomial distribution, which is appropriate for discrete features.\n",
    "Use case: Typically used for text classification problems, where the features represent word counts or term frequencies in documents.\n",
    "Bernoulli Naive Bayes:\n",
    "\n",
    "Assumption: Assumes that the features are binary (0 or 1) and follow a Bernoulli distribution.\n",
    "Use case: Appropriate when dealing with binary features, such as presence or absence of certain attributes.\n",
    "To decide which Naive Bayes classifier to use, consider the nature of your features and their distribution. Here are some general guidelines:\n",
    "\n",
    "Gaussian Naive Bayes is suitable for continuous features that can be assumed to follow a Gaussian distribution, such as when dealing with numerical data.\n",
    "Multinomial Naive Bayes is appropriate when working with discrete features or text data where word frequencies or counts are relevant.\n",
    "Bernoulli Naive Bayes is useful when dealing with binary features, such as presence or absence of certain attributes.\n",
    "However, these are general guidelines, and the choice also depends on the specific characteristics of your dataset and the problem at hand. It is recommended to experiment with different variants and evaluate their performance using suitable evaluation metrics, cross-validation, or other techniques to determine the most appropriate Naive Bayes classifier for your specific problem.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db2e5068-d17d-48f5-867f-613ebacd22a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To predict the class for the new instance with features X1 = 3 and X2 = 4 using Naive Bayes, we need to calculate the posterior probabilities for each class and select the class with the highest probability.\\n\\nGiven that the prior probabilities for class A and class B are equal, we can assume P(A) = P(B) = 0.5.\\n\\nTo calculate the likelihoods for each feature value given the class, we can use the frequency table provided:\\n\\nFor class A:\\nP(X1=3 | A) = 4/12 = 1/3\\nP(X2=4 | A) = 3/12 = 1/4\\n\\nFor class B:\\nP(X1=3 | B) = 1/12\\nP(X2=4 | B) = 3/12 = 1/4\\n\\nNow, let's calculate the posterior probabilities using Bayes' theorem:\\n\\nP(A | X1=3, X2=4) = (P(X1=3 | A) * P(X2=4 | A) * P(A)) / P(X1=3, X2=4)\\nP(B | X1=3, X2=4) = (P(X1=3 | B) * P(X2=4 | B) * P(B)) / P(X1=3, X2=4)\\n\\nSince P(X1=3, X2=4) is the same for both classes, we can ignore it for the purpose of comparison.\\n\\nCalculating the posterior probabilities:\\n\\nP(A | X1=3, X2=4) = (1/3 * 1/4 * 0.5) = 1/24\\nP(B | X1=3, X2=4) = (1/12 * 1/4 * 0.5) = 1/96\\n\\nComparing the posterior probabilities, we can see that P(A | X1=3, X2=4) = 1/24 > P(B | X1=3, X2=4) = 1/96.\\n\\nTherefore, Naive Bayes would predict the new instance to belong to class A.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#6.\n",
    "'''To predict the class for the new instance with features X1 = 3 and X2 = 4 using Naive Bayes, we need to calculate the posterior probabilities for each class and select the class with the highest probability.\n",
    "\n",
    "Given that the prior probabilities for class A and class B are equal, we can assume P(A) = P(B) = 0.5.\n",
    "\n",
    "To calculate the likelihoods for each feature value given the class, we can use the frequency table provided:\n",
    "\n",
    "For class A:\n",
    "P(X1=3 | A) = 4/12 = 1/3\n",
    "P(X2=4 | A) = 3/12 = 1/4\n",
    "\n",
    "For class B:\n",
    "P(X1=3 | B) = 1/12\n",
    "P(X2=4 | B) = 3/12 = 1/4\n",
    "\n",
    "Now, let's calculate the posterior probabilities using Bayes' theorem:\n",
    "\n",
    "P(A | X1=3, X2=4) = (P(X1=3 | A) * P(X2=4 | A) * P(A)) / P(X1=3, X2=4)\n",
    "P(B | X1=3, X2=4) = (P(X1=3 | B) * P(X2=4 | B) * P(B)) / P(X1=3, X2=4)\n",
    "\n",
    "Since P(X1=3, X2=4) is the same for both classes, we can ignore it for the purpose of comparison.\n",
    "\n",
    "Calculating the posterior probabilities:\n",
    "\n",
    "P(A | X1=3, X2=4) = (1/3 * 1/4 * 0.5) = 1/24\n",
    "P(B | X1=3, X2=4) = (1/12 * 1/4 * 0.5) = 1/96\n",
    "\n",
    "Comparing the posterior probabilities, we can see that P(A | X1=3, X2=4) = 1/24 > P(B | X1=3, X2=4) = 1/96.\n",
    "\n",
    "Therefore, Naive Bayes would predict the new instance to belong to class A.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e38a86-3d6f-4a2a-ba9a-52648ed9709c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
